{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNF3BjHL0x_D"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43291490b636"
   },
   "source": [
    "# RAG - Developer Code Chat\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/examples/developer_code_chat/developer_code_chat.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fprompts%2Fexamples%2Fdeveloper_code_chat%2Fdeveloper_code_chat.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/examples/developer_code_chat/developer_code_chat.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/use-cases/retrieval-augmented-generation/developer_code_chat/developer_code_chat.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tF69osTL1aaM"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Charu Shelar](https://github.com/CharulataShelar) |\n",
    "|Reviewer(s) | [Erwin Huizenga](https://github.com/erwinh85) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook showcases the development of an AI-powered learning assistant. This assistant is designed to help programmers or students, learn more about programming languages. The assistant answers users' questions (configured programming languages) using internal documents and Gemini model. It can assist end users with coding tasks, answer questions, and generate code. The solution has been built using the custom RAG approach and Gemini model (Gemini Pro 1.0). It stores the responses in BigQuery. This allows for the caching of more common queries and analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96fad856c458"
   },
   "source": [
    "## As a developer, you will learn the steps to implement the complete solution, i.e. :\n",
    "\n",
    "1. To embed the document and create a vector search index using Vector Search (previously known as Matching Engine).\n",
    "    - Upload new document in GCS bucket\n",
    "    - Separate tab on the UI to allow end users to index newly added documents.\n",
    "    - Python code file used here: developer_code_chatgenerate_embeddings_main.py\n",
    "\n",
    "2. To build RAG (Retrieval-Augmented Generation) for intra document search\n",
    "    - Use Gemini model to allow chat like conversation and retain the session conversation history limited to last N messages (3 previous messages in this case )\n",
    "    - Answer programming questions using indexed documents.\n",
    "    - Answer coding questions using the Gemini model if knowledge base does not have the relevant context/content.\n",
    "    - To prevent hallucinations and maintain appropriate responses, the solution demonstrates how to guardrail the system's response to predetermined programming languages when handling user queries. List of supported programming languages can be configured in config.ini file\n",
    "    - Python code file used here: developer_code_chatdeveloper_code_chatpy\n",
    "\n",
    "4. To build chat UI interface using Gradio\n",
    "5. Integrate BigQuery to save responses\n",
    "    - Save the responses generated by the chatbot agent in Bigquery table for caching and further analytics by session ID: genai-github-assets.genai_data.assistant_feedback\n",
    "    (Note: New session ID is created for each new gradio instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b8bfd66ab54"
   },
   "source": [
    "## GCP services used:\n",
    "\n",
    "1. Vector Search (previously Matching Engine)\n",
    "2. GenAI Model - Gemini Pro 1.0, textembedding-gecko\n",
    "3. BigQuery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88bdc5fac612"
   },
   "source": [
    "## Solution Design Flow\n",
    "\n",
    "![genAI Asset Learning assistent](images/developer_code_chat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab50bce05450"
   },
   "source": [
    "### Data Ingestion Stages:\n",
    "1. Developer team having access to new learning content can upload it to the GCS bucket.\n",
    "\n",
    "2. Data Ingestion Pipeline will fetch documents from the GCS bucket (here \"gs://genai-prod-vme-embedding/references\") and create chunks based on the document sizes.\n",
    "\n",
    "3. Further Data Ingestion Pipeline will get the embeddings for each page using Vertex AI Embeddings model API and index to Vector Search.\n",
    "\n",
    "\n",
    "### Response Generation Stages:\n",
    "1. The user starts a natural language query through a Gradio Chat User Interface (UI).\n",
    "\n",
    "2. Intent classification is done using Gemini model. It classifies the message into one of the following intents: WELCOME, PROGRAMMING_QUESTION_AND_ANSWER, WRITE_CODE, FOLLOWUP, or CLOSE.\n",
    "\n",
    "3. For the WRITE_CODE intent, the Gemini model is used to generate code using its coding capability.\n",
    "\n",
    "4. For the PROGRAMMING_QUESTION_AND_ANSWER intent, custom orchestration (RAG) retrieves context relevant to the user query from Vector Search and summarises relavent contexts. If the answer is not found, the user query is routed to the Gemini Model to respond using its knowledge.\n",
    "\n",
    "5. For the FOLLOWUP intent, such as explaining more or writing code for previous responses, the Gemini Model is used to generate responses using its code capability.\n",
    "\n",
    "6. For the WELCOME and CLOSE intents, the Gemini model is used to generate appropriate responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --user google-cloud-aiplatform \\\n",
    "langchain==0.1.13 \\\n",
    "pypdf==4.1.0 \\\n",
    "gradio==3.41.2 \\\n",
    "langchain-google-vertexai \\\n",
    "--quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0d78ca17444"
   },
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cae7793b5c8"
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import logging\n",
    "import uuid\n",
    "\n",
    "import gradio as gr\n",
    "from utils.generate_embeddings import GenerateEmbeddings\n",
    "from utils.intent_routing import IntentRouting\n",
    "from utils.log_response_bq import LogDetailsInBQ\n",
    "from utils.qna_vector_search import QnAVectorSearch\n",
    "from vertexai.generative_models import GenerativeModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f96bfd65600d"
   },
   "source": [
    "### Update the project settings in config file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f4e674fefb2"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Please do not change the configuration file name i.e. `config.ini` ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9c18fe9c483f"
   },
   "outputs": [],
   "source": [
    "config_file = \"config.ini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b7f3f56c323"
   },
   "source": [
    "#### [One-time] Update the settings in config file\n",
    "\n",
    "**Note:** Some settings in `config.ini` file are are updated from this notebook. \n",
    "Additional parameters can be modified manually or using same code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "088b31c13ccb"
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "config.set(\"default\", \"project_id\", PROJECT_ID)\n",
    "config.set(\"default\", \"region\", LOCATION)\n",
    "\n",
    "with open(config_file, \"w\") as cf:\n",
    "    config.write(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e9087995783"
   },
   "source": [
    "## Logging Conversation Responses in BQ\n",
    "- These logs can be used for caching and further analytics.\n",
    "- If you don't want to store the responses in the BigQuery table, set the **flag_log_response_in_bq** flag to **False**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc752968804a"
   },
   "source": [
    "#### [One-time] Update the settings in config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3860c80af74"
   },
   "outputs": [],
   "source": [
    "flag_log_response_in_bq = True  # @param {type:\"boolean\"}\n",
    "\n",
    "config.set(\n",
    "    \"log_response_in_bq\", \"flag_log_response_in_bq\", str(flag_log_response_in_bq)\n",
    ")\n",
    "\n",
    "with open(config_file, \"w\") as cf:\n",
    "    config.write(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "428d10c6049d"
   },
   "source": [
    "### [One-time] Create BigQuery dataset and table to log conversation responses\n",
    "\n",
    "If above **flag_log_response_in_bq** is set to False, then no need to run cells in below sesion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28cc1b069b2c"
   },
   "outputs": [],
   "source": [
    "DATASET = \"your-bq-dataset-id\"  # @param {type:\"string\"}\n",
    "TABLE = \"your-bq-table-id\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eb31fddf42ed"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "def create_dataset(project_id: str, location: str, dataset_id: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates a new BigQuery dataset if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        location (str): The geographic location where the dataset should reside.\n",
    "        dataset_id (str): The ID of the dataset to create.\n",
    "\n",
    "    Prints status messages indicating success or whether the dataset already existed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Check if the dataset already exists.\n",
    "    try:\n",
    "        client.get_dataset(dataset_id)\n",
    "        print(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except NotFound:\n",
    "        print(\"Dataset {} is not found. Creating new dataset.\".format(dataset_id))\n",
    "\n",
    "        # Construct a full Dataset object to send to the API.\n",
    "        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "\n",
    "        # The geographic location where the dataset should reside.\n",
    "        dataset.location = location\n",
    "\n",
    "        # Send the dataset to the API for creation.\n",
    "        dataset = client.create_dataset(dataset, timeout=30)\n",
    "        print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "\n",
    "create_dataset(PROJECT_ID, LOCATION, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3838b901dc43"
   },
   "outputs": [],
   "source": [
    "def create_table(project_id: str, location: str, dataset_id: str, table_id: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates a new BigQuery table within a specified dataset if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        location (str): The geographic location where the dataset resides.\n",
    "        dataset_id (str): The ID of the dataset in which to create the table.\n",
    "        table_id (str): The ID of the table to create.\n",
    "\n",
    "    Defines a schema for the table with the following fields:\n",
    "        * question (STRING, REQUIRED)\n",
    "        * intent (STRING, REQUIRED)\n",
    "        * assistant_response (STRING, REQUIRED)\n",
    "        * session_id (STRING, REQUIRED)\n",
    "\n",
    "    Prints status messages indicating success or whether the table already existed. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project=project_id, location=location)\n",
    "\n",
    "    # Set table_id to the ID of the table to create.\n",
    "    table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_id)\n",
    "        print(\"Table {} already exists\".format(table_id))\n",
    "    except NotFound:\n",
    "        print(\"Table {} is not found. Creating new table.\".format(dataset_id))\n",
    "\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"question\", \"STRING\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"intent\", \"STRING\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"assistant_response\", \"STRING\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"session_id\", \"STRING\", mode=\"REQUIRED\")\n",
    "        ]\n",
    "\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        table = client.create_table(table)\n",
    "        print(\n",
    "            \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "        )\n",
    "    \n",
    "create_table(PROJECT_ID, LOCATION, DATASET, TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fcaaf767689"
   },
   "outputs": [],
   "source": [
    "bq_table_id = f\"{PROJECT_ID}.{DATASET}.{TABLE}\"\n",
    "config.set(\"log_response_in_bq\", \"bq_table_id\", bq_table_id)\n",
    "\n",
    "with open(config_file, \"w\") as cf:\n",
    "    config.write(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efaf63dffa03"
   },
   "source": [
    "## [On-demand] Setup Vector Search for QnA\n",
    "\n",
    "- Load new document in GSC bucket\n",
    "- Setup a new Vector Search index (create document embeddings, index and deploy the index for semantic search)\n",
    "\n",
    "**Note: This code block to be run only once. If run multiple times, it will re-create the embeddings and update the index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86ce18202484"
   },
   "outputs": [],
   "source": [
    "me_index_name = \"your-vector-search-index-name\"  # @param {type:\"string\"}\n",
    "me_region = \"your-vector-search-index-region\"  # @param {type:\"string\"}\n",
    "\n",
    "me_embedding_region = \"your-gcp-bucket-region\"  # @param {type:\"string\"}\n",
    "me_embedding_dir = \"your-gcp-bucket-name\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bed27a8f1e43"
   },
   "outputs": [],
   "source": [
    "# Set GCP bucket name and location in config file\n",
    "config.set(\"embedding\", \"me_embedding_region\", me_embedding_region)\n",
    "config.set(\"embedding\", \"me_embedding_dir\", me_embedding_dir)\n",
    "\n",
    "# Set Vector search index name and location in config file\n",
    "config.set(\"embedding\", \"me_region\", me_region)\n",
    "config.set(\"embedding\", \"me_index_name\", me_index_name)\n",
    "\n",
    "with open(config_file, \"w\") as cf:\n",
    "    config.write(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db97a1dafa1c"
   },
   "source": [
    "### Upload the document in GCS bucket\n",
    "\n",
    "These documents will be indexed by chunks of pages. Please review the chunk size in the config.ini file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa6ac049c80d"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Upload the document to be indexed in GCS bucket folder path => gs://{config[\"embedding\"][\"me_embedding_dir\"]}/{config[\"embedding\"][\"index_folder_prefix\"]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "296bdb5b7232"
   },
   "source": [
    "### Setup Vector Search index\n",
    "\n",
    "1. Create Vector Search index and Endpoint for Retrieval\n",
    "2. Add Document Embeddings to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21d1b1599bc1"
   },
   "outputs": [],
   "source": [
    "generate_embeddings = GenerateEmbeddings(config_file=config_file)\n",
    "generate_embeddings.generate_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## Chat interface using gradio app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fc5a97ffc91"
   },
   "source": [
    "#### Mount Learning Assistant app with gradio \n",
    "Now, Lets write a chat interface using Gradio that can be used for two tasks i.e. \n",
    "\n",
    "**A. Learning Assistant:**\n",
    "1. QnA using indexed document(s) for configured programming languages. \n",
    "2. If it's not able to retrieve the answer from the first step, then the response is generated from the Gemini model memory as it can  assist end users with coding tasks, generate code and provide suggestion on errors. \n",
    "\n",
    "**B. Data:**\n",
    "1. Select and index documents available in GCP bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85840523aeae"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from google.cloud import storage\n",
    "\n",
    "def get_bucket_content(bucket_or_name: str, prefix: str, config_file: str = \"config.ini\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves details of objects from a Google Cloud Storage bucket with a specified prefix.\n",
    "\n",
    "    Args:\n",
    "        bucket_or_name (str): Name of the GCS bucket to query.\n",
    "        prefix (str):  Prefix to filter objects within the bucket.\n",
    "        config_file (str, optional): Path to the configuration file (default: \"config.ini\").\n",
    "                             Assumes this file is used for GCS credentials. \n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:  A Pandas DataFrame containing the following columns:\n",
    "            * name: Names of the objects.\n",
    "            * size: Object sizes in MB.\n",
    "            * type: Content type of the objects.\n",
    "            * time_created: Timestamps of object creation.\n",
    "            * updated: Timestamps of the last object update.\n",
    "            * storage_class: Storage class of the objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    \n",
    "    client = storage.Client()\n",
    "    data = defaultdict(list)\n",
    "    for blob in client.list_blobs(bucket_or_name=bucket_or_name, prefix=prefix):\n",
    "        data[\"name\"].append(blob.name)\n",
    "        size = blob.size / 1024 / 1024\n",
    "        data[\"size\"].append(f\"{size:.2f} MB\")\n",
    "        data[\"type\"].append(blob.content_type)\n",
    "        data[\"time_created\"].append(str(blob.time_created)\\\n",
    "          .split(\".\", maxsplit=1)[0])\n",
    "        data[\"updated\"].append(str(blob.updated)\\\n",
    "          .split(\".\", maxsplit=1)[0])\n",
    "        data[\"storage_class\"].append(str(blob.storage_class)\\\n",
    "          .split(\".\", maxsplit=1)[0])\n",
    "    df = pd.DataFrame(data=data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjZId8ZSrgMB"
   },
   "outputs": [],
   "source": [
    "def create_app() -> gr.Blocks:\n",
    "    \"\"\"\n",
    "    Initializes and configures the Gradio web interface for the Learning Assistant application.\n",
    "\n",
    "    Key Functions:\n",
    "        * Reads app configuration from 'developer_code_chatconfig.ini'.\n",
    "        * Sets up logging.\n",
    "        * Instantiates core components (IntentRouting, QnAVectorSearch, LogDetailsInBQ).\n",
    "        * Loads language models for intent classification and chat.\n",
    "        * Defines Gradio interface elements, including chatbot and feedback logging.\n",
    "        * Handles user input and orchestrates responses.\n",
    "        * Includes error handling to provide a graceful response in case of exceptions.\n",
    "\n",
    "    Returns:\n",
    "        gr.Blocks: The configured Gradio interface object.\n",
    "        tuple: In case of errors, returns a tuple with an error message, \"ERROR\" string,\n",
    "               and empty answer reference.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Set up logging for the application\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Initialize core components using configuration settings\n",
    "        genai_assistant = IntentRouting(config_file=config_file, logger=logger)\n",
    "        genai_qna = QnAVectorSearch(config_file=config_file, logger=logger)\n",
    "        generate_embeddings = GenerateEmbeddings(config_file=config_file)\n",
    "\n",
    "        # Initialize logging to BigQuery (if configured)\n",
    "        bq_logger = None\n",
    "        if bool(config[\"log_response_in_bq\"][\"flag_log_response_in_bq\"]):\n",
    "            bq_logger = LogDetailsInBQ(config_file=config_file)\n",
    "\n",
    "        # Load language models for QnA and conversational interaction\n",
    "        model = GenerativeModel(config[\"genai_qna\"][\"model_name\"])\n",
    "        chat_model = GenerativeModel(config[\"genai_chat\"][\"model_name\"])\n",
    "\n",
    "        # Start the chat session and provide initial instructions to the chatbot\n",
    "        default_programming_language = config[\"default\"][\"default_language\"]\n",
    "        chat = chat_model.start_chat(history=[])\n",
    "        _ = chat.send_message(\n",
    "            f\"\"\"You are a Programming Language Learning Assistant.\n",
    "                Your task is to undersand the question and respond with the descriptive answer for the same.\n",
    "\n",
    "                Instructions:\n",
    "                1. If programming language is not mentioned, then use {default_programming_language} as default programming language to write a code.\n",
    "                2. Strictly follow the instructions mentioned in the question.\n",
    "                3. If the question is not clear then you can answer \"I apologize, but I am not able to understand the question. Please try to elaborate and rephrase your question.\"\n",
    "\n",
    "                If the question is about other programming language then DO NOT provide any answer, just say \"I apologize, but I am not able to understand the question. Please try to elaborate and rephrase your question.\"\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        # Define Gradio interface elements\n",
    "        with gr.Blocks() as demo:\n",
    "            with gr.Tab(\"Learning Assistant\"):\n",
    "                # Welcome message for the chatbot\n",
    "                bot_message = \"Hi there! I'm Generative AI powered Learning Assistant. I can help you with coding tasks, answer questions, and generate code. Just ask me anything you need, and I'll do my best to help!\"  # pylint: disable=C0301:line-too-long\n",
    "\n",
    "                # Generate a unique session identifier\n",
    "                session_state = str(uuid.uuid4())\n",
    "                logger.info(\"session_state : %s\", session_state)\n",
    "\n",
    "                # Configure the chatbot UI element\n",
    "                chatbot = gr.Chatbot(\n",
    "                    height=600,\n",
    "                    label=\"\",  # No display label for the chatbot\n",
    "                    value=[[None, bot_message]],  # Initialize with the welcome message\n",
    "                    avatar_images=(\n",
    "                        None,\n",
    "                        \"https://fonts.gstatic.com/s/i/short-term/release/googlesymbols/smart_assistant/default/24px.svg\",\n",
    "                    ),  # Assistant avatar\n",
    "                    elem_classes=\"message\",\n",
    "                    show_label=False,\n",
    "                )\n",
    "\n",
    "                msg = gr.Textbox(\n",
    "                    scale=4,\n",
    "                    label=\"\",\n",
    "                    placeholder=\"Enter your question here..\",\n",
    "                    elem_classes=[\"form\", \"message-row\"],\n",
    "                )\n",
    "\n",
    "                def respond(message, chat_history):\n",
    "                    \"\"\"Sending response to gradio\"\"\"\n",
    "                    # intent\n",
    "                    (\n",
    "                        response,\n",
    "                        intent,\n",
    "                        answer_reference,\n",
    "                    ) = genai_assistant.classify_intent(\n",
    "                        message,\n",
    "                        session_state,\n",
    "                        model,\n",
    "                        chat_model,\n",
    "                        genai_qna,\n",
    "                    )\n",
    "\n",
    "                    # append response to history\n",
    "                    chat_history.append((message, response + answer_reference))\n",
    "\n",
    "                    if bq_logger:\n",
    "                        bq_logger.save_response(\n",
    "                            question=message,\n",
    "                            intent=intent,\n",
    "                            response=response + answer_reference,\n",
    "                            session_state=session_state,\n",
    "                        )\n",
    "\n",
    "                    return \"\", chat_history\n",
    "\n",
    "                msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "            with gr.Tab(\"Data\"):\n",
    "                data_df = get_bucket_content(\n",
    "                    bucket_or_name=config[\"embedding\"][\"me_embedding_dir\"],\n",
    "                    prefix=config[\"embedding\"][\"index_folder_prefix\"],\n",
    "                )\n",
    "                # data\n",
    "                _ = gr.Dataframe(data_df)\n",
    "                progress = gr.Textbox(label=\"Index Document Status\")\n",
    "                btn = gr.Button(value=\"Index Documents\")\n",
    "                btn.click(generate_embeddings.generate_embeddings, outputs=[progress])\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "\n",
    "        print(\"Error : \", e)\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "        return (\n",
    "            \"We're sorry, but we encountered a problem. Please try again.\",\n",
    "            \"ERROR\",\n",
    "            \"\",\n",
    "        )\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4fff6052d41"
   },
   "source": [
    "#### Launch the gradio app to view the chatbot\n",
    "\n",
    "**Note:**\n",
    "1. For a better experience, Open the demo application interface in a new tab by clicking on the Localhost url generated after running this cell.\n",
    "2. For debugging mode, set `debug=True`\n",
    "\n",
    "\n",
    "**Sample Questions to try on UI**\n",
    "1. Where can we use python programming language?\n",
    "2. What is the difference between list and set?\n",
    "3. Fix the error in below code:\n",
    "\n",
    "def create_dataset(id: str): -> None\n",
    "...\n",
    "\n",
    "SyntaxError: invalid syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5b32aa361209"
   },
   "outputs": [],
   "source": [
    "demo = create_app()\n",
    "demo.queue().launch(share=True, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35553c14b7f8"
   },
   "source": [
    "### Close the demo\n",
    "\n",
    "**Note:** Stop the previous cell to close the Gradio server running, then run this cell to free up the port utilised for running the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "704dbb0c853b"
   },
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "developer_code_chat.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
