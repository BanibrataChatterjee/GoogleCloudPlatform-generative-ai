{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5u9Hn4LkqND6"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI6OrHr9R6AA"
   },
   "source": [
    "# Multimodal Retrieval Augmented Generation (RAG) using Vertex AI Gemini API\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/multimodal_rag_QnA_usecase.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fretrieval-augmented-generation%2Fmultimodal_rag_QnA_usecase.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/multimodal_rag_QnA_usecase.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/multimodal_rag_QnA_usecase.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3K6twUOR-jD"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Aakash Gouda](https://github.com/aksstar) , [Bhushan Garware](https://github.com/BhushanGarware)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INIA40pMSYOn"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Retrieval augmented generation (RAG) has become a popular paradigm for enabling LLMs to access external data and also as a mechanism for grounding to mitigate against hallucinations.\n",
    "\n",
    "In this notebook, you will learn how to perform RAG where you will perform Q&A over a document filled with both text and images.\n",
    "\n",
    "### Gemini\n",
    "\n",
    "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases. The Gemini API gives you access to the Gemini 1.0 Pro Vision and Gemini 1.0 Pro models.\n",
    "\n",
    "### Comparing text-based and multimodal RAG\n",
    "\n",
    "Multimodal RAG offers several advantages over text-based RAG:\n",
    "\n",
    "1. **Enhanced knowledge access:** Multimodal RAG can access and process both textual and visual information, providing a richer and more comprehensive knowledge base for the LLM.\n",
    "2. **Improved reasoning capabilities:** By incorporating visual cues, multimodal RAG can make better informed inferences across different types of data modalities.\n",
    "\n",
    "This notebook shows you how to use multimodal RAG with Vertex AI Gemini API, [text embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings) to build a question answering system for a PDF document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0M72MEscSasQ"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VEOKEsLSdTx"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "This notebook provides a guide to building a questions answering system using multimodal retrieval augmented generation (RAG).\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "1. Extract data from documents containing both text and images using Gemini Vision Pro, and generate embeddings of the data, store it in vector store\n",
    "2. Search the vector store with text queries to find similar text data\n",
    "3. Using Text data as context, generate answer to the user query using Gemini Pro Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r11Gu7qNgx1p"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hfn_UeXyTHaH"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet pymupdf langchain gradio google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7UyNVSiyQ96"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After its restarted, continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmY9HVVGSBW5"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXQZrM5hQeKb"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Wait for the kernel to finish restarting before you continue. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the cell below to authenticate your environment.\n",
    "\n",
    "This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Define Google Cloud project information and initialize Vertex AI\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "# Define project information\n",
    "# PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "PROJECT_ID = \"aakash-test-env\"\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXHfaVS66_01"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lslYAvw37JGQ"
   },
   "outputs": [],
   "source": [
    "# File system operations and displaying images\n",
    "import os\n",
    "# Libraries for downloading files, data manipulation, and creating a user interface\n",
    "import urllib.request\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "# Initialize Vertex AI libraries for working with generative models\n",
    "from vertexai.generative_models import GenerativeModel, Image\n",
    "\n",
    "# Create a \"utils\" directory if it doesn't exist\n",
    "if not os.path.exists(\"utils\"):\n",
    "    os.makedirs(\"utils\")\n",
    "\n",
    "# Download utility files from a GitHub repository\n",
    "url_prefix = \"https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/use-cases/document-qa/utils\"\n",
    "files = [\"__init__.py\", \"matching_engine.py\", \"matching_engine_utils.py\"]\n",
    "\n",
    "for fname in files:\n",
    "    urllib.request.urlretrieve(f\"{url_prefix}/{fname}\", filename=f\"utils/{fname}\")\n",
    "\n",
    "# Import utility functions for timing and file handling\n",
    "import time\n",
    "\n",
    "import fitz\n",
    "# Print Vertex AI SDK version\n",
    "from google.cloud import aiplatform\n",
    "from PIL import Image as PIL_Image\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
    "\n",
    "# Import LangChain components\n",
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "# Import custom vector search packages\n",
    "from utils.matching_engine import MatchingEngine\n",
    "from utils.matching_engine_utils import MatchingEngineUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lP1EMJL4ddcA"
   },
   "source": [
    "### Initalizing Gemini Vision Pro and Text Embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNOqAHc3dbDf"
   },
   "outputs": [],
   "source": [
    "# Loading Gemini Model\n",
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")\n",
    "\n",
    "# Initalizing embedding model\n",
    "embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOZpB1hCtYtQ"
   },
   "source": [
    "### Download a sample PDF file from internet [Skip this step if you have uploaded your PDF file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCuvbLu9tDOa"
   },
   "outputs": [],
   "source": [
    "!wget https://www.hitachi.com/rev/archive/2023/r2023_04/pdf/04a02.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPQ5eqaTcLP7"
   },
   "source": [
    "### Split PDF pages to Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-41JZgAwsWXi"
   },
   "outputs": [],
   "source": [
    "# Run the following code for each file\n",
    "PDF_FILENAME = \"04a02.pdf\"  # <-- Replace with your filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSKL45w4O5ob"
   },
   "outputs": [],
   "source": [
    "# Create an \"Images\" directory if it doesn't exist\n",
    "Image_Path = \"./Images/\"\n",
    "if not os.path.exists(Image_Path):\n",
    "    os.makedirs(Image_Path)\n",
    "\n",
    "# To get better resolution\n",
    "zoom_x = 2.0  # horizontal zoom\n",
    "zoom_y = 2.0  # vertical zoom\n",
    "mat = fitz.Matrix(zoom_x, zoom_y)  # zoom factor 2 in each dimension\n",
    "\n",
    "doc = fitz.open(PDF_FILENAME)  # open document\n",
    "for page in doc:  # iterate through the pages\n",
    "    pix = page.get_pixmap(matrix=mat)  # render page to an image\n",
    "    outpath = f\"./Images/{PDF_FILENAME}_{page.number}.jpg\"\n",
    "    pix.save(outpath)  # store image as a PNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNaBV6iSuleV"
   },
   "source": [
    "This module processes a set of images, extracting text and tabular data using a multimodal model (Gemini Vision Pro).\n",
    "It handles potential errors, stores the extracted information in a DataFrame, and saves the results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qU9kcKXdXsUH"
   },
   "outputs": [],
   "source": [
    "# Define the path where images are located\n",
    "image_names = os.listdir(Image_Path)\n",
    "Max_images = len(image_names)\n",
    "\n",
    "# Create empty lists to store image information\n",
    "page_source = []\n",
    "page_content = []\n",
    "page_id = []\n",
    "\n",
    "p_id = 0  # Initialize image ID counter\n",
    "rest_count = 0  # Initialize counter for error handling\n",
    "\n",
    "while p_id < Max_images:\n",
    "    try:\n",
    "        # Construct the full path to the current image\n",
    "        image_path = Image_Path + image_names[p_id]\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.load_from_file(image_path)\n",
    "\n",
    "        # Generate prompts for text and table extraction\n",
    "        prompt_text = \"Extract all text content in the image\"\n",
    "        prompt_table = (\n",
    "            \"Detect table in this image. Extract content maintaining the structure\"\n",
    "        )\n",
    "\n",
    "        # Extract text using your multimodal model\n",
    "        contents = [image, prompt_text]\n",
    "        response = multimodal_model.generate_content(contents)\n",
    "        text_content = response.text\n",
    "\n",
    "        # Extract table using your multimodal model\n",
    "        contents = [image, prompt_table]\n",
    "        response = multimodal_model.generate_content(contents)\n",
    "        table_content = response.text\n",
    "\n",
    "        # Log progress and store results\n",
    "        print(f\"processed image no: {p_id}\")\n",
    "        page_source.append(image_path)\n",
    "        page_content.append(text_content + \"\\n\" + table_content)\n",
    "        page_id.append(p_id)\n",
    "        p_id += 1\n",
    "\n",
    "    except Exception as err:\n",
    "        # Handle errors during processing\n",
    "        print(err)\n",
    "        print(\"Taking Some Rest\")\n",
    "        time.sleep(1)  # Pause execution for 1 second\n",
    "        rest_count += 1\n",
    "        if rest_count == 5:  # Limit consecutive error handling\n",
    "            rest_count = 0\n",
    "            print(f\"Can not process image no: {image_path}\")\n",
    "            p_id += 1  # Move to the next image\n",
    "\n",
    "# Create a DataFrame to store extracted information\n",
    "df = pd.DataFrame(\n",
    "    {\"page_id\": page_id, \"page_source\": page_source, \"page_content\": page_content}\n",
    ")\n",
    "del page_id, page_source, page_content  # Conserve memory\n",
    "print(df.head())  # Preview the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgOJQHEBhhbz"
   },
   "source": [
    "### Creating Vertex AI: Vector Search\n",
    "The code configures and deploys a vector search index on Google Cloud, making it ready to store and search through embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtCRJeKsYc6w"
   },
   "outputs": [],
   "source": [
    "VECTOR_SEARCH_REGION = \"us-central1\"\n",
    "VECTOR_SEARCH_INDEX_NAME = f\"{PROJECT_ID}-vector-search-index-ht\"\n",
    "VECTOR_SEARCH_EMBEDDING_DIR = f\"{PROJECT_ID}-vector-search-bucket-ht\"\n",
    "VECTOR_SEARCH_DIMENSIONS = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dzj_FcL3YeXW"
   },
   "outputs": [],
   "source": [
    "# Creates a GCS bucket for storing vector search embedding\n",
    "! set -x && gsutil mb -p $PROJECT_ID -l us-central1 gs://$VECTOR_SEARCH_EMBEDDING_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAmL3qCQYhYx"
   },
   "outputs": [],
   "source": [
    "# Initalizing vector search from utils\n",
    "vector_search = MatchingEngineUtils(\n",
    "    PROJECT_ID, VECTOR_SEARCH_REGION, VECTOR_SEARCH_INDEX_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOLu1KqkYi-7"
   },
   "outputs": [],
   "source": [
    "# Create and Deploy Vector Search Index\n",
    "index = vector_search.create_index(\n",
    "    embedding_gcs_uri=f\"gs://{VECTOR_SEARCH_EMBEDDING_DIR}/init_index\",\n",
    "    dimensions=VECTOR_SEARCH_DIMENSIONS,\n",
    "    index_update_method=\"streaming\",\n",
    "    index_algorithm=\"tree-ah\",\n",
    ")\n",
    "if index:\n",
    "    print(index.name)\n",
    "\n",
    "index_endpoint = vector_search.deploy_index()\n",
    "if index_endpoint:\n",
    "    print(f\"Index endpoint resource name: {index_endpoint.name}\")\n",
    "    print(\n",
    "        f\"Index endpoint public domain name: {index_endpoint.public_endpoint_domain_name}\"\n",
    "    )\n",
    "    print(\"Deployed indexes on the index endpoint:\")\n",
    "    for d in index_endpoint.deployed_indexes:\n",
    "        print(f\"    {d.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgvdovAqdboX"
   },
   "source": [
    "This code snippet prepares textual data for storage in a vector search engine. The end goal is to enable efficient similarity-based search queries against this textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruENZMXuYonq"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrameLoader to prepare data for LangChain\n",
    "loader = DataFrameLoader(df, page_content_column=\"page_content\")\n",
    "\n",
    "# Load documents from the 'page_content' column of your DataFrame\n",
    "documents = loader.load()\n",
    "\n",
    "# Log the number of documents loaded\n",
    "print(f\"# of documents loaded (pre-chunking) = {len(documents)}\")\n",
    "\n",
    "# Create a text splitter to divide documents into smaller chunks\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=10000,  # Target size of approximately 10000 characters per chunk\n",
    "    chunk_overlap=0,  # No overlap between chunks\n",
    ")\n",
    "\n",
    "# Split the loaded documents\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add a 'chunk' ID to each document split's metadata for tracking\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"chunk\"] = idx\n",
    "\n",
    "# Log the number of documents after splitting\n",
    "print(f\"# of documents = {len(doc_splits)}\")\n",
    "\n",
    "\n",
    "# Retrieve Vector Search index and endpoint IDs (a helper function 'vector_search.get_index_and_endpoint()')\n",
    "(\n",
    "    VECTOR_SEARCH_INDEX_ID,\n",
    "    VECTOR_SEARCH_ENDPOINT_ID,\n",
    ") = vector_search.get_index_and_endpoint()\n",
    "\n",
    "# Print the retrieved IDs for reference\n",
    "print(f\"VECTOR_SEARCH_INDEX_ID={VECTOR_SEARCH_INDEX_ID}\")\n",
    "print(f\"VECTOR_SEARCH_INDEX_ENDPOINT_ID={VECTOR_SEARCH_ENDPOINT_ID}\")\n",
    "\n",
    "# Initialize a Vector Search object for managing your vector search system\n",
    "vector_store_object = MatchingEngine.from_components(\n",
    "    project_id=PROJECT_ID,  # Your Google Cloud Project ID\n",
    "    region=VECTOR_SEARCH_REGION,  # Region where your vector search index is located\n",
    "    gcs_bucket_name=f\"gs://{VECTOR_SEARCH_EMBEDDING_DIR}\".split(\"/\")[\n",
    "        2\n",
    "    ],  # GCS bucket storing embeddings\n",
    "    embedding=embeddings,  # Your embedding model\n",
    "    index_id=VECTOR_SEARCH_INDEX_ID,\n",
    "    endpoint_id=VECTOR_SEARCH_ENDPOINT_ID,\n",
    ")\n",
    "\n",
    "# Prepare lists of text content and metadata\n",
    "texts = [doc.page_content for doc in doc_splits]\n",
    "metadatas = [\n",
    "    [\n",
    "        {\n",
    "            \"namespace\": \"source\",\n",
    "            \"allow_list\": [doc.metadata[\"page_source\"]],\n",
    "        },  # Metadata to track the original source\n",
    "        {\n",
    "            \"namespace\": \"chunk\",\n",
    "            \"allow_list\": [str(doc.metadata[\"chunk\"])],\n",
    "        },  # Metadata to track the chunk number\n",
    "    ]\n",
    "    for doc in doc_splits\n",
    "]\n",
    "\n",
    "# Embed the text chunks and store them in the vector search index\n",
    "doc_ids = vector_store_object.add_texts(texts=texts, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM3a9RQx4pQO"
   },
   "source": [
    "### Ask Questions to the PDF\n",
    "This code snippet establishes a question-answering (QA) system.  It leverages a vector search engine to find relevant information from a dataset and then uses the 'gemini-pro' LLM model to generate and refine the final answer to a user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRme7I2hhqC8"
   },
   "outputs": [],
   "source": [
    "# LLM model initalization for Retrieval Chain\n",
    "gemini_pro_model = VertexAI(\n",
    "    model_name=\"gemini-pro\",  # Name of the LLM model on Vertex AI\n",
    "    max_output_tokens=1024,  # Limits the maximum response length\n",
    "    temperature=0,  # Controls randomness (lower = more deterministic)\n",
    "    top_p=0.8,\n",
    "    top_k=40,  # Sampling parameters for text generation\n",
    "    verbose=True,  # Enables logging\n",
    ")\n",
    "\n",
    "\n",
    "def Test_LLM_Response(txt):\n",
    "    \"\"\"\n",
    "    Determines whether a given text response generated by an LLM indicates a lack of information.\n",
    "\n",
    "    Args:\n",
    "        txt (str): The text response generated by the LLM.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the LLM's response suggests it was able to generate a meaningful answer,\n",
    "              False if the response indicates it could not find relevant information.\n",
    "\n",
    "    This function works by presenting a formatted classification prompt to the LLM (`gemini_pro_model`).\n",
    "    The prompt includes the original text and specific categories indicating whether sufficient information was available.\n",
    "    The function analyzes the LLM's classification output to make the determination.\n",
    "    \"\"\"\n",
    "\n",
    "    classification_prompt = f\"\"\" Classify the text as one of the following categories:\n",
    "        -Information Present\n",
    "        -Infromation Not Present\n",
    "        Text=The provided context does not contain information.\n",
    "        Category:Infromation Not Present\n",
    "        Text=I cannot answer this question from the provided context.\n",
    "        Category:Infromation Not Present\n",
    "        Text:{txt}\n",
    "        Category:\"\"\"\n",
    "\n",
    "    if gemini_pro_model(classification_prompt) == \" Information Not Present\":\n",
    "        return False  # Indicates that the LLM couldn't provide an answer\n",
    "    else:\n",
    "        return True  # Suggests the LLM generated a meaningful response\n",
    "\n",
    "\n",
    "def get_answer(query):\n",
    "    \"\"\"\n",
    "    Retrieves an answer to a provided query using a vector search system and an LLM.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the LLM-generated answer ('result' key) and potentially other relevant metadata.\n",
    "\n",
    "    This function coordinates the following steps:\n",
    "\n",
    "    1. **Document Retrieval:** Fetches relevant documents from a vector search system (`vector_store_object`) based on similarity to the query.\n",
    "    2. **Answer Generation:** Leverages an LLM (`gemini_pro_model`) within a LangChain RetrievalQA chain to generate an answer from the retrieved documents.\n",
    "    3. **Quality Check:**  Employs the `Test_LLM_Response` function as a filter to assess the quality of the generated answer.\n",
    "    4. **Iteration:**  Iteratively increases the number of retrieved documents (`k`) if the answer quality is insufficient, up to a maximum of 5 attempts.\n",
    "    5. **Error Handling:** Handles potential exceptions, aiming to return the best available result.\n",
    "    \"\"\"\n",
    "    k = 1\n",
    "    flag = 0\n",
    "    while flag == 0:\n",
    "        try:\n",
    "            # Configure retriever using your vector search system\n",
    "            retriever = vector_store_object.as_retriever(\n",
    "                search_type=\"similarity\", search_kwargs={\"k\": k}\n",
    "            )\n",
    "            # Setup a RetrievalQA chain (fetch documents + use LLM)\n",
    "            qa = RetrievalQA.from_chain_type(\n",
    "                llm=gemini_pro_model,\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=retriever,\n",
    "                return_source_documents=True,\n",
    "            )\n",
    "            result = qa({\"query\": query})\n",
    "            txt = result[\"result\"]\n",
    "            if not Test_LLM_Response(txt):\n",
    "                k = k + 1  # Increase the number of retrieved documents if needed\n",
    "            else:\n",
    "                flag = 1  # Exit loop when getting a valid response\n",
    "            if k == 5:\n",
    "                flag = 1  # Limit attempts\n",
    "        except:\n",
    "            k = k - 1\n",
    "            if k == 0:\n",
    "                return result  # Return the best available result on error\n",
    "    return result\n",
    "\n",
    "\n",
    "question = (\n",
    "    \"what is the 5th step of Transformer Manufacturing Flow ?\"  # @param {type:\"string\"}\n",
    ")\n",
    "print(get_answer(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeIDns8neIdO"
   },
   "source": [
    "# Ask Questions to the PDF using Gradio UI\n",
    " this code creates a web-based frontend for your question-answering system, allowing users to easily enter queries and see the results along with relevant images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dYlueAKbCJe"
   },
   "outputs": [],
   "source": [
    "def gradio_query(query):\n",
    "\n",
    "    # Retrieve the answer from your QA system\n",
    "    result = get_answer(query)\n",
    "    answer = result[\"result\"]\n",
    "\n",
    "    try:\n",
    "        # Attempt to fetch the source image reference\n",
    "        ref = result[\"source_documents\"][-1].metadata[\"source\"]\n",
    "        image = PIL_Image.open(ref)  # Open the reference image\n",
    "    except:\n",
    "        # Use a default image if the reference is not found [PLEASE ADD A DEFAULT IMAGE]\n",
    "        image = PIL_Image.open(\"./Images/blank.jpg\")\n",
    "\n",
    "    return [answer, image]  # Return both the text answer and the image\n",
    "\n",
    "\n",
    "gr.close_all()  # Ensure a clean Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            # Input / Output Components\n",
    "            query = gr.Textbox(label=\"Query\", info=\"Enter your query\")\n",
    "            btn_enter = gr.Button(\"Process\")\n",
    "            answer = gr.Textbox(label=\"Response\", info=\"Enter your query\")\n",
    "            btn_clear = gr.Button(\"Clear\")\n",
    "        with gr.Column():\n",
    "            image = gr.Image(label=\"Refrence\", visible=True)\n",
    "\n",
    "    # Button Click Event\n",
    "    btn_enter.click(fn=gradio_query, inputs=query, outputs=[answer, image])\n",
    "\n",
    "demo.launch(share=True, debug=True)  # Launch the Gradio app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUac1wDXPDC2"
   },
   "source": [
    "### Cleaning up\n",
    "To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItrB6IIUCMb1"
   },
   "outputs": [],
   "source": [
    "# Deleting vector search index and endpoint\n",
    "vector_search.delete_index_endpoint()\n",
    "vector_search.delete_index()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "multimodal_rag_langchain[Q&A_usecase].ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
