{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNF3BjHL0x_D"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF69osTL1aaM"
      },
      "source": [
        "# RAG - Developer Code Chat\n",
        "\n",
        "# Text Classification with Generative Models on Vertex AI\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/examples/developer_code_chat.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fprompts%2Fexamples%2Fdeveloper_code_chat.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/examples/developer_code_chat.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/use-cases/retrieval-augmented-generation/developer_code_chat.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEL97Al7HfBZ"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Charu Shelar](https://github.com/CharulataShelar) |\n",
        "|Reviewer(s) | [Erwin Huizenga](https://github.com/erwinh85) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook showcases the development of an AI-powered learning assistant. This assistant is designed to help users, such as programmers or students, learn more about programming languages. The assistant answers users' questions using internal documents. It can assist end users with coding tasks, answer questions, and generate code. The solution has been built using the custom RAG approach and Gemini model (Gemini Pro 1.0). It stores the responses in BigQuery. This allows for the caching of more common queries and analytics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvH9oQAv7rKv"
      },
      "source": [
        "## As a developer, you will learn the steps to implement the complete solution, i.e. :\n",
        "\n",
        "1. To embed the document and create a vector search index using Vector Search (previously known as Matching Engine).\n",
        "Upload new document in GCS bucket\n",
        "Separate tab on the UI to allow end users to index newly added documents.\n",
        "Python code file used here: learning_assistant/generate_embeddings_main.py\n",
        "\n",
        "2. To build RAG (Retrieval-Augmented Generation) for intra document search\n",
        "Use Gemini model to allow chat like conversation and retain the session conversation history limited to last N messages (3 previous messages in this case )\n",
        "Answer programming questions using indexed documents.\n",
        "Answer coding questions using the Gemini model if knowledge base does not have the relevant context/content\n",
        "To prevent hallucinations and maintain appropriate responses, the solution demonstrates how to guardrail the system's response to predetermined programming languages when handling user queries. List of supported programming languages can be configured in config.ini file\n",
        "Python code file used here: learning_assistant/learning_assistant.py\n",
        "\n",
        "4. To build chat UI interface using Gradio\n",
        "5. Integrate BigQuery to save responses\n",
        "Save the responses generated by the chatbot agent in Bigquery table for caching and further analytics by session ID: genai-github-assets.genai_data.assistant_feedback\n",
        "(Note: New session ID is created for each new gradio instance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KTNxgfP7whe"
      },
      "source": [
        "## GCP services used:\n",
        "\n",
        "1. Vector Search (previously Matching Engine)\n",
        "2. GenAI Model - Gemini Pro 1.0, textembedding-gecko\n",
        "3. BigQuery\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE84_K1_YBQl"
      },
      "source": [
        "## Solution Design Flow\n",
        "\n",
        "![genAI Asset Learning assistent](https://screenshot.googleplex.com/A7voRwxx6RRm9sV.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzMEXeQIoiyv"
      },
      "source": [
        "### Data Ingestion Stages:\n",
        "1. Developer team having access to new learning content can upload it to the GCS bucket.\n",
        "\n",
        "2. Data Ingestion Pipeline will fetch documents from the GCS bucket (here \"gs://genai-prod-vme-embedding/references\") and create chunks based on the document sizes.\n",
        "\n",
        "3. Further Data Ingestion Pipeline will get the embeddings for each page using Vertex AI Embeddings model API and index to Vector Search.\n",
        "\n",
        "\n",
        "### Response Generation Stages:\n",
        "1. The user starts a natural language query through a Gradio Chat User Interface (UI).\n",
        "\n",
        "2. Intent classification is done using Gemini model. It classifies the message into one of the following intents: WELCOME, PROGRAMMING_QUESTION_AND_ANSWER, WRITE_CODE, FOLLOWUP, or CLOSE.\n",
        "\n",
        "3. For the WRITE_CODE intent, the Gemini model is used to generate code using its coding capability.\n",
        "\n",
        "4. For the PROGRAMMING_QUESTION_AND_ANSWER intent, custom orchestration (RAG) retrieves context relevant to the user query from Vector Search and summarises relavent contexts. If the answer is not found, the user query is routed to the Gemini Model to respond using its knowledge.\n",
        "\n",
        "5. For the FOLLOWUP intent, such as explaining more or writing code for previous responses, the Gemini Model is used to generate responses using its code capability.\n",
        "\n",
        "6. For the WELCOME and CLOSE intents, the Gemini model is used to generate appropriate responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "# !pip3 install --upgrade --user --quiet google-cloud-aiplatform\n",
        "# !pip install gradio\n",
        "# !pip install langchain\n",
        "# !pip install pypdf\n",
        "# !pip install -U langchain-google-vertexai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "\n",
        "# if \"google.colab\" in sys.modules:\n",
        "#     import IPython\n",
        "\n",
        "#     app = IPython.Application.instance()\n",
        "#     app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD6ncn_hxpf5"
      },
      "source": [
        "### Clone the source code from CSR repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "133YqYOFwQAi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rag_developer_code_chat folder already exists\n",
            "Cloning into '/content/rag_developer_code_chat'...\n",
            "remote: Finding sources: 100% (5/5)\u001b[K\n",
            "remote: Total 65 (delta 31), reused 62 (delta 31)\u001b[K\n",
            "Receiving objects: 100% (65/65), 66.94 KiB | 3.52 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n",
            "Project [genai-github-assets] repository [rag_developer_code_chat] was cloned to [/content/rag_developer_code_chat].\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if os.path.isdir(\"rag_developer_code_chat\"):\n",
        "    # If the folder already exists:\n",
        "    print(\"rag_developer_code_chat folder already exists\")\n",
        "\n",
        "    # 1. Either pull the latest repo code in the folder before running the notebook\n",
        "    # !git reset --hard\n",
        "    # !git pull\n",
        "\n",
        "    # 2. or remove entire folder and clone the repo again\n",
        "    !rm -r rag_developer_code_chat -f\n",
        "    !gcloud source repos clone rag_developer_code_chat --project=genai-github-assets\n",
        "else:\n",
        "    !gcloud source repos clone rag_developer_code_chat --project=genai-github-assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgyYQoFp1lns"
      },
      "source": [
        "### Change the working directory to cloned folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9tEhEKTzWg0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default working directory :  /content\n",
            "Working directory changed to :  /content/rag_developer_code_chat\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"Default working directory : \", os.getcwd())\n",
        "\n",
        "path = \"rag_developer_code_chat\"\n",
        "os.chdir(path)\n",
        "print(\"Working directory changed to : \", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1VlYe5o2Svv"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRbtSg2ktyCJ"
      },
      "outputs": [],
      "source": [
        "import configparser\n",
        "import logging\n",
        "import uuid\n",
        "\n",
        "import gradio as gr\n",
        "from learning_assistant.generate_embeddings_main import index_documents\n",
        "from learning_assistant.learning_assistant import get_bucket_content\n",
        "from learning_assistant.utils.intent_routing import IntentRouting\n",
        "from learning_assistant.utils.log_response_bq import LogDetailsInBQ\n",
        "from learning_assistant.utils.qna_vector_search import QnAVectorSearch\n",
        "from learning_assistant.utils.vertex_matching_engine_utils import \\\n",
        "    deploy_index_endpoint\n",
        "from vertexai.generative_models import GenerativeModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"genai-github-assets\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## Chat interface using gradio app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snlwLch1w0q7"
      },
      "source": [
        "#### Mount Learning Assistant app with gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjZId8ZSrgMB"
      },
      "outputs": [],
      "source": [
        "def create_app() -> gr.Blocks:\n",
        "    \"\"\"\n",
        "    Initializes and configures the Gradio web interface for the Learning Assistant application.\n",
        "\n",
        "    Key Functions:\n",
        "        * Reads app configuration from 'learning_assistant/config.ini'.\n",
        "        * Sets up logging.\n",
        "        * Deploys the index endpoint (if necessary).\n",
        "        * Instantiates core components: IntentRouting, QnAVectorSearch, LogDetailsInBQ.\n",
        "        * Loads language models for intent classification and chat.\n",
        "        * Defines Gradio interface elements, including chatbot and feedback logging.\n",
        "        * Handles user input and orchestrates responses.\n",
        "\n",
        "    Returns:\n",
        "        gr.Blocks: The configured Gradio interface object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Settings\n",
        "    config_file = \"learning_assistant/config.ini\"\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(config_file)\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logger = logging.getLogger(__name__)\n",
        "    deploy_index_endpoint(logger)\n",
        "\n",
        "    genai_assistant = IntentRouting(config_file=config_file, logger=logger)\n",
        "    genai_qna = QnAVectorSearch(config_file=config_file, logger=logger)\n",
        "    bq_logger = LogDetailsInBQ(config_file=config_file)\n",
        "\n",
        "    model = GenerativeModel(config[\"model_parameter_classify\"][\"classify_model_name\"])\n",
        "\n",
        "    chat_model = GenerativeModel(\n",
        "        config[\"model_parameter_classify\"][\"classify_model_name\"]\n",
        "    )\n",
        "    chat = chat_model.start_chat(history=[])\n",
        "    _ = chat.send_message(\n",
        "        \"\"\"You are Programming Language Learning Assistant.\n",
        "            Your task is to undersand the question and answer descriptive answer for the same.\n",
        "\n",
        "            Instructions:\n",
        "            1. If programming language is not mentioned, then use java as default programming language to write a code.\n",
        "            2. Strictly follow the instructions mentioned in the question.\n",
        "            3. If the question is not clear then you can answer \"I apologize, but I am not able to understand the question. Please try to elaborate and rephrase your question.\"\n",
        "\n",
        "            If the question is about other programming language then DO NOT provide any answer, just say \"I apologize, but I am not able to understand the question. Please try to elaborate and rephrase your question.\"\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    css_code = \"\"\"\n",
        "    .form {border-color: #ecedfc !important}\n",
        "    .user {border-color: #f9fafb !important;background-color: #fbfbff !important}\n",
        "    .bot {border-color: #f9fafb !important;background-color: #f7f0ff !important}\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(css=css_code) as demo:\n",
        "        with gr.Tab(\"Learning Assistant\"):\n",
        "            bot_message = \"Hi there! I'm Generative AI powered Learning Assistant. I can help you with coding tasks, answer questions, and generate code. Just ask me anything you need, and I'll do my best to help!\"  # pylint: disable=C0301:line-too-long\n",
        "\n",
        "            session_state = str(uuid.uuid4())  # \"test\"\n",
        "            logger.info(\"session_state : %s\", session_state)\n",
        "\n",
        "            chatbot = gr.Chatbot(\n",
        "                height=600,\n",
        "                label=\"\",\n",
        "                value=[[None, bot_message]],\n",
        "                avatar_images=(\n",
        "                    None,\n",
        "                    \"https://fonts.gstatic.com/s/i/short-term/release/googlesymbols/smart_assistant/default/24px.svg\",\n",
        "                ),  # # pylint: disable=C0301:line-too-long\n",
        "                elem_classes=\"message\",\n",
        "                show_label=False,\n",
        "            )\n",
        "            msg = gr.Textbox(\n",
        "                scale=4,\n",
        "                label=\"\",\n",
        "                placeholder=\"Enter your question here..\",\n",
        "                elem_classes=[\"form\", \"message-row\"],\n",
        "            )\n",
        "\n",
        "            def respond(\n",
        "                message, chat_history\n",
        "            ):  # pylint: disable=W0621:redefined-outer-name\n",
        "                \"\"\"Sending response to gradio\"\"\"\n",
        "                # intent\n",
        "                (response, intent, answer_reference,) = genai_assistant.classify_intent(\n",
        "                    message,\n",
        "                    session_state,\n",
        "                    model,\n",
        "                    chat_model,\n",
        "                    chat_history,\n",
        "                    genai_qna,\n",
        "                )\n",
        "\n",
        "                # append response to history\n",
        "                chat_history.append((message, response + answer_reference))\n",
        "\n",
        "                bq_logger.save_response(\n",
        "                    question=message,\n",
        "                    intent=intent,\n",
        "                    response=response + answer_reference,\n",
        "                    session_state=session_state,\n",
        "                )\n",
        "\n",
        "                return \"\", chat_history\n",
        "\n",
        "            msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "        with gr.Tab(\"Data\"):\n",
        "            data_df = get_bucket_content()\n",
        "            # data\n",
        "            _ = gr.Dataframe(data_df)\n",
        "            progress = gr.Textbox(label=\"Index Document Status\")\n",
        "            btn = gr.Button(value=\"Index Documents\")\n",
        "            btn.click(index_documents, outputs=[progress])\n",
        "\n",
        "    return demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZwHThmDwxjN"
      },
      "source": [
        "#### Launch the gradio app to view the chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt2fjZPGtyCK"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://a77de6cc4e8869f051.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a77de6cc4e8869f051.gradio.live\" width=\"80%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.embeddings.vertexai:Model_name will become a required arg for VertexAIEmbeddings starting from Feb-01-2024. Currently the default is set to textembedding-gecko@001\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "        SYSTEM: You are genai Programming Language Learning Assistant helping the students answer their questions based on following context. Explain the answers in detail for students.\n",
            "\n",
            "        Instructions:\n",
            "        1. Think step-by-step and then answer.\n",
            "        2. Explain the answer in detail.\n",
            "        3. If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
            "        4. If the context is empty, just say \"I could not find any references that are directly related to your question.\"\n",
            "\n",
            "        Context:\n",
            "        =============\n",
            "        \n",
            "        =============\n",
            "\n",
            "        What is the Detailed explanation of answer of following question?\n",
            "        Question: can you help identify difference between list and set?\n",
            "        Detailed explanation of Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.embeddings.vertexai:Model_name will become a required arg for VertexAIEmbeddings starting from Feb-01-2024. Currently the default is set to textembedding-gecko@001\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "        SYSTEM: You are genai Programming Language Learning Assistant helping the students answer their questions based on following context. Explain the answers in detail for students.\n",
            "\n",
            "        Instructions:\n",
            "        1. Think step-by-step and then answer.\n",
            "        2. Explain the answer in detail.\n",
            "        3. If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
            "        4. If the context is empty, just say \"I could not find any references that are directly related to your question.\"\n",
            "\n",
            "        Context:\n",
            "        =============\n",
            "        \n",
            "        =============\n",
            "\n",
            "        What is the Detailed explanation of answer of following question?\n",
            "        Question: examples of list\n",
            "        Detailed explanation of Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "demo = create_app()\n",
        "demo.launch(width=\"80%\", share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "developer_code_chat.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
