# Attacks and Mitigation Labs 
 **Prompt, RAG and ReAct**

 This comprehensive learning CoLabs dives deep into the critical world of LLM security, equipping you with the knowledge and skills to build, deploy, and maintain secure and trustworthy AI solutions. From understanding the latest attack vectors to implementing cutting-edge defenses, this CoLabs is your guide to navigating the evolving landscape of LLM security.


- LLM Prompt attacks and mitigation 

  CoLab [GenAI_&_LLM_Security_(v2_Gemini)_For_Developers_Prompt_Attacks,_Mitigation_Techniques_&_Protection_Strategies.ipynb](colabs/GenAI_&_LLM_Security_(v2_Gemini)_For_Developers_Prompt_Attacks,_Mitigation_Techniques_&_Protection_Strategies.ipynb)
  - Simple prompt design prompt design
  - Antipatterns on prompt design with PII data and secrets
  - Prompt Attacks:
    - Data Leaking
    - Data Leaking with Transformations
    - Modifying the Output (Jailbreaking)
    - Hallucinations
    - Payload Splitting
    - Virtualization
    - Obfuscation
    - Multimodal Attacks (Image, PDF & Video)
    - Model poisioning
  - Protections & Mitigations with:
    - Data Loss Prevention
    - Natural Language API (Category Check, Sentiment Analysis)
    - Malware checking
    - LLM validation (Hypothesis Validation, DARE, Strict Input Validation with Random Token)
    - Responsible AI Safety filters
    - Embeddings

- ReAct and RAG attacks and mitigation options [GenAI_and_LLM_Security_ReAct_and_RAG_attacks_&_mitigations.ipynb](GenAI_and_LLM_Security_ReAct_and_RAG_attacks_&_mitigations.ipynb)
  - Simple ReAct applicaiton design 
  - Attacks using ReAct Agent 
  - Mitigation using ReAct Agent information
  - Simple RAG applicaiton 
  - Attacks and mitigation for RAG applications





