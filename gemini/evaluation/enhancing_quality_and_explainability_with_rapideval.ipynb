{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# Enhancing quality and explainability with Vertex RapidEval\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/enhancing_quality_and_explainability_with_rapideval.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/enhancing_quality_and_explainability_with_rapideval.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fevaluation%2Fenhancing_quality_and_explainability_with_rapideval.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/enhancing_quality_and_explainability_with_rapideval.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce84bd67392c"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Anant Nawalgaria](https://github.com/anantnawal) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkHPv2myT2cx"
   },
   "source": [
    "## Overview\n",
    "\n",
    "### Gemini\n",
    "\n",
    "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases. The Gemini API gives you access to the Gemini Pro Vision and Gemini Pro models.\n",
    "\n",
    "### Vertex Rapid eval API\n",
    "\n",
    "The [Rapid Evaluation Service](https://cloud.google.com/vertex-ai/generative-ai/docs/models/rapid-evaluation) which can be accessed both through its SDK and web API interfaces,  lets you evaluate your large language models (LLMs), both pointwise and pairwise, across several metrics. It is part of the [Vertex AI Evaluation Service](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval), which also allows you to evaluate models and prompts in an offline fashion.\n",
    "\n",
    "Rapid eval is primarily used ad-hoc in the initial experimental phase for evaluating which set of prompts and models work well for a use case. However, as described in detail in the corresponding blog, this notebook will show some sample code on dummy data of how you can use Rapid Evaluation to enhance the quality of the response generated by the LLMs by combining the pairwise and pointwise capabilities of Rapid Evaluation elegantly at the time of generation. It would also then return a human readable explanation to help understand the quality evaluation of the response. Note that although this notebook only demonstrates this workflow on text, it can be extended to any modality once an evaluation mechanism is available for that modality.\n",
    "\n",
    "For more information about generative AI on Vertex AI please see [Generative AI on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview) documentation.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrkcqHrrwMAo"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this tutorial, you will learn how to combine the Vertex AI Gemini API  with the Rapid Eval API service for Python to improve generation quality & explainability of the responses.\n",
    "You will complete the following tasks:\n",
    "\n",
    "- Install the Vertex AI SDK for Python\n",
    "- Use the Vertex AI Gemini API to interact with each model\n",
    "  - Gemini Pro (`gemini-1.5-pro`) model:\n",
    "    - Generate multiple responses for a given instruction and context\n",
    "    - Use the pairwise and pointwise capabilities of Rapid eval to select the best response and also return a human readable explanation for it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9nEPojogw-g"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r11Gu7qNgx1p"
   },
   "source": [
    "## Getting Started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install the required libraries for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --user google-cloud-aiplatform\n",
    "!pip3 install --upgrade --user --quiet google-cloud-aiplatform[rapid_evaluation]\n",
    "!pip3 install \"bigframes<1.0.0\" -q\n",
    "!pip3 install --quiet --upgrade nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7UyNVSiyQ96"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, it is recommended to restart the runtime. Run the following cell to restart the current kernel.\n",
    "\n",
    "The restart process might take a minute or so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmY9HVVGSBW5"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b21ea7cdbf7"
   },
   "source": [
    "#### Set your project ID and region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14cc8984ed02"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"visual-search-poc-gcp\"\n",
    "LOCATION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea53274ede4d"
   },
   "source": [
    "After the restart is complete, continue to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXQZrM5hQeKb"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Wait for the kernel to finish restarting before you continue. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Define Google Cloud project information (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, specify the Google Cloud project information to use. In the following cell, you specify your project information, import the Vertex AI package, and initialize the package. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "if \"google.colab\" in sys.modules:\n",
    "    # Define project information\n",
    "    # Initialize Vertex AI\n",
    "    import vertexai\n",
    "\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXHfaVS66_01"
   },
   "source": [
    "### Import libraries & initialize project variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lslYAvw37JGQ"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import uuid\n",
    "from functools import partial\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from google import auth\n",
    "from google.auth.transport import requests as google_auth_requests\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.preview.evaluation import EvalTask\n",
    "from vertexai.preview.generative_models import GenerationConfig, GenerativeModel\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39ce45a03542"
   },
   "source": [
    "## Definining functions for ranking using evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "This section defines the various helper functions to perform pairwise and pointwise evaluations, as well as the logic to combine them \n",
    "to select the best response and return associated quality metrics and explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afc4054f1f70"
   },
   "source": [
    "The function below enable Auto SxS comparisons between pairs of responses using the Rapid Evaluation Web API to be used in conjunction with the python max or sorted inbuilt functions  in order to use pairwise comparions to find the best response or get a ranked list of responses respectively.\n",
    "The SDK could also be used for this purpose once it supports this functionality. The complete list of metrics for other tasks like summarization SxS can be found on the website linked below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac10531efd0c"
   },
   "outputs": [],
   "source": [
    "def pairwise_greater(instructions, context, project_id, location, baseline, candidate):\n",
    "    \"\"\"\n",
    "    Takes Instructions, Context and two different responses.\n",
    "    Returns the response which best matches the instructionds/Context for the given\n",
    "    quality metric ( in this case question answering).\n",
    "    More details on the web API and different quality metrics which this function\n",
    "    can be extended to can be found on\n",
    "    https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/evaluation\n",
    "    \"\"\"\n",
    "    creds, _ = auth.default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "\n",
    "    data = {\n",
    "        \"pairwise_question_answering_quality_input\": {\n",
    "            \"metric_spec\": {},\n",
    "            \"instance\": {\n",
    "                \"prediction\": candidate,\n",
    "                \"baseline_prediction\": baseline,\n",
    "                \"instruction\": instructions,\n",
    "                \"context\": (context),\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "    uri = f\"https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{project_id}/locations/{location}:evaluateInstances\"\n",
    "    result = google_auth_requests.AuthorizedSession(creds).post(uri, json=data)\n",
    "    result = result.json()[\"pairwiseQuestionAnsweringQualityResult\"]\n",
    "    choice = baseline if result[\"pairwiseChoice\"] == \"BASELINE\" else candidate\n",
    "    return choice, result[\"explanation\"], result[\"confidence\"]\n",
    "\n",
    "\n",
    "def greater(cmp, a, b):\n",
    "    \"\"\"\n",
    "    A comparison function which takes the comparison function, and two variables as input\n",
    "    and returns the one which is greater according to the logic defined inside the cmp function.\n",
    "    \"\"\"\n",
    "    choice, explanation, confidence = cmp(a, b)\n",
    "\n",
    "    if choice == a:\n",
    "        return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "602441b87ef7"
   },
   "source": [
    "The below function performs the pointwise evaluation of the provided set of responses, with respect to the provided metric, instruction and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6863af9c235f"
   },
   "outputs": [],
   "source": [
    "experiment_name = \"qa_pointwise\"\n",
    "\n",
    "\n",
    "def pointwise_eval(\n",
    "    instruction,\n",
    "    context,\n",
    "    responses,\n",
    "    eval_metrics=[\n",
    "        \"question_answering_quality\",\n",
    "        \"question_answering_helpfulness\",\n",
    "        \"question_answering_relevance\",\n",
    "    ],\n",
    "    experiment_name=experiment_name,\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes the instruction, context and a variable number of corresponding generated responses, and returns the pointwise evaluation metrics\n",
    "    for each of the provided metrics. For this example the metrics are Q & A related, however the full list can be found on the website:\n",
    "    https://cloud.google.com/vertex-ai/generative-ai/docs/models/online-pipeline-services\n",
    "    \"\"\"\n",
    "\n",
    "    instructions = [instruction] * len(responses)\n",
    "\n",
    "    contexts = [context] * len(responses)\n",
    "\n",
    "    eval_dataset = pd.DataFrame(\n",
    "        {\n",
    "            \"instruction\": instructions,\n",
    "            \"context\": contexts,\n",
    "            \"response\": responses,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    eval_task = EvalTask(\n",
    "        dataset=eval_dataset, metrics=eval_metrics, experiment=experiment_name\n",
    "    )\n",
    "    results = eval_task.evaluate(experiment_run_name=\"gemini-qa\" + str(uuid.uuid4()))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "971acda8dd12"
   },
   "source": [
    "The function below combines the pairwise and pointwise logic into one workflow. It first does pairwise comparisons to rank and select the best response\n",
    "w.r.t the user specified metric and then uses pointwise evaluation to get quality metrics and human readable explanation of why the user \n",
    "should trust that response. However the two steps can also be reversed: by performing pointwise evaluation of the metrics first, filtering out all\n",
    "those responses where certain quality criteria is not met, and then taking the top N best performing metrics and ranking them with the pairwise methods\n",
    "in order to arrive at the best response(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ac1c136a334"
   },
   "outputs": [],
   "source": [
    "def rank_responses(instruction, context, responses):\n",
    "    \"\"\"\n",
    "    Takes the instruction, context and a variable number of responses as input, and returns the best performing response as well as its associated\n",
    "    human readable pointwise quality metrics for the configured criteria in the above functions.\n",
    "    The process consists of two steps:\n",
    "    1. Selecting the best response by using Pairwise comparisons between the responses for the user specified metric ( e.g. Q & A)\n",
    "    2. Doing pointwise evaluation of the best response and returning human readable quality metrics and explanation along with the best response.\n",
    "    \"\"\"\n",
    "    cmp_f = partial(pairwise_greater, instruction, context, PROJECT_ID, LOCATION)\n",
    "    cmp_greater = partial(greater, cmp_f)\n",
    "\n",
    "    pairwise_best_response = max(responses, key=functools.cmp_to_key(cmp_greater))\n",
    "    pointwise_metric = pointwise_eval(instruction, context, [pairwise_best_response])\n",
    "    qa_metrics = pointwise_metric.metrics_table[\n",
    "        [\n",
    "            col\n",
    "            for col in pointwise_metric.metrics_table.columns\n",
    "            if (\"question_answering\" in col) or (\"groundedness\" in col)\n",
    "        ]\n",
    "    ].to_dict(\"records\")[0]\n",
    "\n",
    "    return pairwise_best_response, qa_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY1nfXrqRxVX"
   },
   "source": [
    "### Load the Gemini Pro model\n",
    "Here we load the gemini 1.5 pro model, and assign a tmeperaure value in the range 0.2 to 0.6. A higher temperature value is critical This is important even for use cases where creativity is less important like Q & A: since de-correlated responses would mean if the model gets it wrong with the top choice for one response, it has a possibility of getting it right with one of the other responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e761e89b9065"
   },
   "outputs": [],
   "source": [
    "generation_model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.4, max_output_tokens=512, candidate_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8669160b92b9"
   },
   "source": [
    "### Prompt gemini\n",
    "Now we prompt gemini to generate multiple slightly de-corelated responses based on the above config. Multiple Responses can also be generated in single call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fb5e3ae9faf"
   },
   "outputs": [],
   "source": [
    "instruction_qa = \"Please answer the following question based on the context provided. Question: what is the correct process of fixing your tires?\"\n",
    "context_qa = (\n",
    "    \"Context:\\n\"\n",
    "    + \"the world is a magical place and fixing tires is one of those magical tasks. According to the Administration and Association (TIA), the only method to properly repair a tire puncture is to fill the injury with a repair stem and back the stem with a repair patch. This is commonly known as a combination repair or a patch/plug repair.\"\n",
    ")\n",
    "prompt_qa = instruction_qa + \"\\n\" + context_qa + \"\\n\\nAnswer:\\n\"\n",
    "responses: List[str] = []\n",
    "num_responses = 5\n",
    "for i in range(num_responses):\n",
    "    responses.append(\n",
    "        generation_model.generate_content(\n",
    "            contents=prompt_qa,\n",
    "            generation_config=generation_config,\n",
    "        ).text\n",
    "    )\n",
    "prompt_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c54c44f3f0b5"
   },
   "source": [
    "Here we use the `rank_responses()` function to fetch the best selected response as well as its associated quality metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20230a87d5c1"
   },
   "outputs": [],
   "source": [
    "best_response, metrics = rank_responses(instruction_qa, context_qa, responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b5b63a55144"
   },
   "source": [
    "Now we print the various generated responses:\n",
    "1. The raw responses generated by Gemini\n",
    "2. The best performing response\n",
    "3. Its associated pointwise quality metrics and explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ba246f23c1f"
   },
   "outputs": [],
   "source": [
    "for i in range(len(responses)):\n",
    "    print(\"Response no. {}: \\n {}\".format(i + 1, responses[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8321faa562f0"
   },
   "outputs": [],
   "source": [
    "print(best_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88458eafb3f9"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2871493acdd3"
   },
   "source": [
    "## Cleaning Up\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86bbf7dc97ff"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "experiment = aiplatform.Experiment(experiment_name)\n",
    "experiment.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "enhancing_quality_and_explainability_with_rapideval.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
