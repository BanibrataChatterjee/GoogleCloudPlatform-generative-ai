{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "metadata": {
    "id": "z3Gi-zdCeEbE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Document Q&A With Retrieval Augmented Generation\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/search/retrieval-augmented-generation/examples/rag_google_documentation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/search/retrieval-augmented-generation/examples/rag_google_documentation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/search/retrieval-augmented-generation/examples/rag_google_documentation.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ],
   "metadata": {
    "id": "WcfbGhs1eJF6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook demonstrates how to implement Retrieval Augmented Generation with basic automated evaluation. It demonstrates the impact that chunk size, overlap and context length have on model outputs. The notebook will create a Q&A system that allows you to find information based on the Google Cloud Generative AI documentation."
   ],
   "metadata": {
    "id": "ghQ2aBsbnMyn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting started"
   ],
   "metadata": {
    "id": "DsW5tPDRkT4m"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install libraries"
   ],
   "metadata": {
    "id": "jx1FQVAokWVb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --user google-cloud-aiplatform==1.35.0"
   ],
   "metadata": {
    "id": "nJFw23w1kYVj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ],
   "metadata": {
    "id": "zD-MQab5nWJ-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ],
   "metadata": {
    "id": "GcW5P0sSnXeP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n",
    "\n"
   ],
   "metadata": {
    "id": "i8azCoQenZDq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, you will need to authenticate your environment. To do this, run the new cell below. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
   ],
   "metadata": {
    "id": "-jwsaMQYkZm8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Define project information\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "    LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ],
   "metadata": {
    "id": "ikOmH4doxOFs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import libraries"
   ],
   "metadata": {
    "id": "-h0ba4rmkpKW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLUml_s7iqBc"
   },
   "outputs": [],
   "source": [
    "import requests, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg\n",
    "from google.api_core import retry\n",
    "from vertexai.language_models import TextEmbeddingModel, TextGenerationModel\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scrape text from Google Cloud documentation"
   ],
   "metadata": {
    "id": "TKBmi2BMk_OU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Retrieve list of Google documentation URLs from a text file"
   ],
   "metadata": {
    "id": "OXG6N0WclGsQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with open(\"URLs.txt\", \"r\") as f:\n",
    "    URLS = [line.strip() for line in f.readlines()]"
   ],
   "metadata": {
    "id": "tXHmC10IitET"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parse the HTML and extract relevant plain text sections"
   ],
   "metadata": {
    "id": "u-Ly0yNVlReK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Given a Google documentation URL, retrieve a list of all text chunks within h2 sections\n",
    "def get_sections(url: str) -> list[str]:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    result = []\n",
    "    body_div = soup.find(\"div\", {\"class\": \"devsite-article-body\"})\n",
    "    inner_snips = []\n",
    "    for child in body_div.findChildren():\n",
    "        if child.name == \"p\":\n",
    "            inner_snips.append(child.get_text().strip())\n",
    "        if child.name == \"h2\":\n",
    "            result.append(\" \".join(inner_snips))\n",
    "            break\n",
    "\n",
    "    for header in soup.find_all(\"h2\"):\n",
    "        inner_snips = []\n",
    "        nextNode = header\n",
    "        while True:\n",
    "            nextNode = nextNode.nextSibling\n",
    "            if nextNode is None:\n",
    "                break\n",
    "            if isinstance(nextNode, Tag):\n",
    "                if nextNode.name == \"p\":\n",
    "                    inner_snips.append(nextNode.get_text().strip())\n",
    "                if nextNode.name == \"ul\":\n",
    "                    inner_snips.append(nextNode.get_text().strip())\n",
    "                if nextNode.name == \"h2\":\n",
    "                    result.append(\" \".join(inner_snips))\n",
    "                    break\n",
    "    return result"
   ],
   "metadata": {
    "id": "hMD6Qz_TkFMG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "all_text = [t for url in URLS for t in get_sections(url) if t]"
   ],
   "metadata": {
    "id": "poNdlLf4kFp5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that most documents are relatively short, but some are thousands of characters long"
   ],
   "metadata": {
    "id": "Wy-qw-xslYpX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text_lengths = []\n",
    "for t in all_text:\n",
    "    text_lengths.append(len(t))\n",
    "pd.DataFrame(text_lengths).hist()"
   ],
   "metadata": {
    "id": "DSkdu30tuNbY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create vector store\n",
    "\n",
    "Start by initializing the models"
   ],
   "metadata": {
    "id": "r00cIHIVlj4E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "embeddings_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "text_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ],
   "metadata": {
    "id": "D26RnssLln3U"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create some helper functions for vector similarity and chunking"
   ],
   "metadata": {
    "id": "eEYwgmPxlokS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Separates seq into multiple chunks in the specified size with the specified overlap\n",
    "def split_overlap(seq, size, overlap):\n",
    "    if len(seq) <= size:\n",
    "        return [seq]\n",
    "    return [\"\".join(x) for x in zip(*[seq[i :: size - overlap] for i in range(size)])]\n",
    "\n",
    "\n",
    "# Compute the cosine similarity of two vectors, wrap as returned function to make easier to use with Pandas\n",
    "def get_similarity_fn(query_vector):\n",
    "    def fn(row):\n",
    "        return np.dot(row, query_vector) / (\n",
    "            numpy.linalg.norm(row) * numpy.linalg.norm(query_vector)\n",
    "        )\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "# Retrieve embeddings from the specified model with retry logic\n",
    "@retry.Retry(timeout=300.0)\n",
    "def get_embeddings(text):\n",
    "    return embeddings_model.get_embeddings([text])[0].values"
   ],
   "metadata": {
    "id": "SStUcSPluhvw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the vector store, we are using a Pandas DataFrame"
   ],
   "metadata": {
    "id": "70aXFPhJmCM8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def create_vector_store(texts, chunk_size, overlap):\n",
    "    vector_store = pd.DataFrame()\n",
    "    # Insert the individual texts into the vector store\n",
    "    vector_store[\"texts\"] = list(\n",
    "        itertools.chain(*[split_overlap(t, chunk_size, overlap) for t in texts])\n",
    "    )\n",
    "\n",
    "    # Create embeddings from those texts\n",
    "    vector_store[\"embeddings\"] = vector_store[\"texts\"].progress_apply(get_embeddings)\n",
    "    vector_store[\"embeddings\"] = vector_store[\"embeddings\"].apply(np.array)\n",
    "\n",
    "    return vector_store"
   ],
   "metadata": {
    "id": "0cEJeeGIgFxc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "CHUNK_SIZE = 400\n",
    "OVERLAP = 50\n",
    "\n",
    "vector_store = create_vector_store(all_text, CHUNK_SIZE, OVERLAP)"
   ],
   "metadata": {
    "id": "ifp-Y_kryXJ3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vector_store.head()"
   ],
   "metadata": {
    "id": "ORlMIcEw0LVW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Search the vector store and use for generation"
   ],
   "metadata": {
    "id": "GAJZc3mamQli"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we send the question to the foundation model alone, it will hallucinate."
   ],
   "metadata": {
    "id": "bdNIXBUimv01"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text_model.predict(\n",
    "    \"How long will a stable model version of text-bison be available?\"\n",
    ").text"
   ],
   "metadata": {
    "id": "QEJKQz5ymw1f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's solve this problem by retrieving texts from our vector store and telling the model to use them.\n",
    "\n",
    "Search the vector store for relevant texts to insert into the prompt by embedding the query and searching for similar vectors."
   ],
   "metadata": {
    "id": "mZbX1dkAnB6V"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_context(question, vector_store, num_docs):\n",
    "    # Embed the search query\n",
    "    query_vector = np.array(get_embeddings(question))\n",
    "\n",
    "    # Get similarity to all other vectors and sort, cut off at num_docs\n",
    "    top_matched = (\n",
    "        vector_store[\"embeddings\"]\n",
    "        .apply(get_similarity_fn(query_vector))\n",
    "        .sort_values(ascending=False)[:num_docs]\n",
    "        .index\n",
    "    )\n",
    "    top_matched_df = vector_store[vector_store.index.isin(top_matched)][[\"texts\"]]\n",
    "\n",
    "    # Return a string with the top matches\n",
    "    context = \" \".join(top_matched_df.texts.values)\n",
    "    return context"
   ],
   "metadata": {
    "id": "csMpD6498FXL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a prompt that includes the context and question. Instruct the LLM to only use the context provided to answer the question"
   ],
   "metadata": {
    "id": "W6kDwMEAmnfl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def answer_question(question, vector_store, num_docs=10, print_prompt=False):\n",
    "    context = get_context(question, vector_store, num_docs)\n",
    "    qa_prompt = f\"\"\"Your mission is to answer questions based on a given context. Remember that before you give an answer, you must check to see if it complies with your mission.\n",
    "Context: ```{context}```\n",
    "Question: ***{question}***\n",
    "Before you give an answer, make sure it is only from information in the context. If the information is not in the context, just reply \"I don't know the answer to that\". Think step by step.\n",
    "Answer: \"\"\"\n",
    "    if print_prompt:\n",
    "        print(qa_prompt)\n",
    "    result = text_model.predict(qa_prompt, temperature=0)\n",
    "    return result.text"
   ],
   "metadata": {
    "id": "KfZnJF470esv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the fully generated prompt, the context is embedded. Even though the input context is quite messy, the model can now answer factually."
   ],
   "metadata": {
    "id": "96kS0ZU-m6W6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "answer_question(\n",
    "    \"How long will a stable model version of text-bison be available?\",\n",
    "    vector_store,\n",
    "    print_prompt=True,\n",
    ")"
   ],
   "metadata": {
    "id": "90dMoKTr066y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "answer_question(\n",
    "    \"How long will a stable model version of text-bison be available?\", vector_store\n",
    ")"
   ],
   "metadata": {
    "id": "bmfEIvKmnmCb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Automated evaluation\n",
    "\n",
    "This implementation of RAG is dependent on the chunk size, the overlap between the chunks, the number of texts passed into the context and the prompt. Let's create a simple prompt to evaluate answers to the questions, this will allow us to tweak the parameters and see how those tweaks compare."
   ],
   "metadata": {
    "id": "k2A5mQ6Znvmz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def eval_answer(question, answer, context):\n",
    "    eval_prompt = f\"\"\"Your mission is to evaluate answers to questions based on a given context. Remember that before you give an answer, you must check to see if it complies with your mission.\n",
    "\n",
    "Context: ```{context}```\n",
    "Question: ***{question}***\n",
    "Answer: \"{answer}\"\n",
    "\n",
    "Respond only with a number from 0 to 5. Think step by step. If the provided answer is not in the context, reply 5 if it is \"I don't know the answer to that\" otherwise reply 0.\n",
    "Relevance: \"\"\"\n",
    "    # Stop sequence to cut the model off after outputting an integer\n",
    "    result = text_model.predict(\n",
    "        eval_prompt, temperature=0, max_output_tokens=1, stop_sequences=[\".\", \" \"]\n",
    "    )\n",
    "    return int(result.text)"
   ],
   "metadata": {
    "id": "UB5wB4NR2COn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pass several questions in and retrieve the evaluations"
   ],
   "metadata": {
    "id": "fVMJ9gBPoU-k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    \"What release stage is the RLHF tuning feature?\",\n",
    "    \"Can I generate hate speech with text bison?\",\n",
    "    \"What format should my batch prediction in put be in?\",\n",
    "    \"How can I get the number of tokens?\",\n",
    "    \"How do I create a custom style model?\",\n",
    "    \"What is the dimensionality of the vector created by the multimodal model?\",\n",
    "    \"How long will a stable model verison be available?\",\n",
    "]"
   ],
   "metadata": {
    "id": "NyLMJ0u42yxY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "answers = [answer_question(q, vector_store) for q in questions]\n",
    "contexts = [get_context(q, vector_store, 10) for q in questions]\n",
    "idks = [\"I don't know\" in a for a in answers]\n",
    "evals = [\n",
    "    (question, answer, context, eval_answer(question, answer, context), idk)\n",
    "    for question, answer, context, idk in zip(questions, answers, contexts, idks)\n",
    "]"
   ],
   "metadata": {
    "id": "BftOPiMKFm_8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pd.DataFrame(evals, columns=[\"question\", \"answer\", \"context\", \"score\", \"idk\"])"
   ],
   "metadata": {
    "id": "Zb7VfarNF9W1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now adjust the parameters and see the difference in performance"
   ],
   "metadata": {
    "id": "J_X2OjzsodzI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def eval_on_params(chunk_size, overlap, num_docs):\n",
    "    vector_store = create_vector_store(all_text, chunk_size, overlap)\n",
    "    answers = [answer_question(q, vector_store) for q in questions]\n",
    "    contexts = [get_context(q, vector_store, num_docs) for q in questions]\n",
    "    idks = [\"I don't know\" in a for a in answers]\n",
    "    evals = [\n",
    "        (question, answer, context, eval_answer(question, answer, context), idk)\n",
    "        for question, answer, context, idk in zip(questions, answers, contexts, idks)\n",
    "    ]\n",
    "    return pd.DataFrame(\n",
    "        evals, columns=[\"question\", \"answer\", \"context\", \"score\", \"idk\"]\n",
    "    )"
   ],
   "metadata": {
    "id": "tWKZent6MNf4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Smaller chunk sizes takes longer to generate the embeddings"
   ],
   "metadata": {
    "id": "92rq4ZGerqNX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "smaller_context_df = eval_on_params(100, 0, 5)"
   ],
   "metadata": {
    "id": "zuweYEbVgoZt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "smaller_context_df"
   ],
   "metadata": {
    "id": "J38F4YZpi1Bf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "A larger context size has created more unknowns. When composing LLMs into systems, carefully consider how to measure the performance of each component in the sytem."
   ],
   "metadata": {
    "id": "zQC7gWPWokJO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "larger_context_df = eval_on_params(1000, 200, 15)"
   ],
   "metadata": {
    "id": "4jmWGmzdgwUI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "larger_context_df"
   ],
   "metadata": {
    "id": "BNOwnFQwizBb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "xxVrXRR5jRU_"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}