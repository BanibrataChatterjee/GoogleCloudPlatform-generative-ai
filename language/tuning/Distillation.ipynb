{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3710d312-cbbe-4fc1-9db6-712e8bccddd9",
   "metadata": {},
   "source": [
    "# Distillation step-by-step\n",
    "### A step-by-step guide\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/distilling_step_by_step/Distillation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/distilling_step_by_step/Distillation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/distilling_step_by_step/Distillation.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be8b8d-6e73-427f-a420-4e23a0001ad8",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14832e-4aab-4051-b37f-1718db87c252",
   "metadata": {},
   "source": [
    "**Distillation** is a technique in machine learning that allows us to extract the learnings of a large model and represent it using a smaller model. This allows for improved scalability, as the smaller model requires less resources to run and less time to generate inferences while still achieving accuracy close to that of the larger model.\n",
    "\n",
    "Traditionally, distillation uses the internal parameters of the larger model (specifically, the logits) to train the smaller model. However, some of the best performing large language models today, including Google's (PaLM 2)[https://ai.google/discover/palm2/] model, are exposed to consumers as an API, with no means to access the internal parameters. Until recently, this has prohibited the use of these models as teacher models for distillation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c72c65c-b6b5-47ac-915b-f1a19c5cf312",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "In this notebook, we will go over the technique described in the paper [Distilling step-by-step](https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html), which describes a novel approach to distill the knowledge of a large LLM into a smaller LLM without requiring the internal parameters of the larger model. The original code from the research is available at [https://github.com/google-research/distilling-step-by-step](https://github.com/google-research/distilling-step-by-step).\n",
    "\n",
    "We will go through each step of training a small (student) model to mimic the reasoning ability of a larger (teacher) model. By training the student model to mimic the reasoning ability rather than the actual outputs, we can make the smaller model generalize better to other unseen inputs.\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Preparing a dataset for distillation\n",
    "- Setting up a distillation pipeline\n",
    "- Training a student model using PaLM as a teacher model\n",
    "- Evaluating the distilled model's performance\n",
    "- Deploying the distilled model to Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d4286f-c81f-4b5d-8bed-b2eb1daf550c",
   "metadata": {},
   "source": [
    "## Costs\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI Pipelines\n",
    "- Vertex AI Model Hosting\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b40ab-9b54-4435-aa50-5746381cd4e7",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce302a-67c3-41b4-80ed-4d48d783170a",
   "metadata": {},
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ce182-e072-4576-9c6c-08652b3f1e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install -y pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch\n",
    "! pip install git+https://github.com/huggingface/transformers@v4.24.0 datasets sentencepiece protobuf==3.20.* tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f998a431-4219-4716-a61a-b7ebe48cace5",
   "metadata": {},
   "source": [
    "# Step 1: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2eb53-9454-46ac-b3a4-5b01e93ba78e",
   "metadata": {},
   "source": [
    "Our dataset will need three fields -\n",
    "1. An input prompt for the LLM\n",
    "2. A ground truth label, which is the expected output\n",
    "3. A 'rationale', which is the reasoning generated by the teacher model (using CoT prompting)\n",
    "\n",
    "Here, we will use the [Common Sense Explanations](https://huggingface.co/datasets/cos_e) dataset from HuggingFace to train our student model. This dataset contains around 10k training samples and 1.2k test samples. We will use pre-generated rationales from the PaLM model as a teacher, and we will preprocess the dataset to fit the above schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebefb0de-17b8-4f41-b56a-f3afdf21e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "from typing import Dict, Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe43325-f2c7-4627-a561-1fe7aeb7828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DATASET = \"cos_e\"\n",
    "SOURCE_DATASET_VERSION = \"v1.11\"\n",
    "\n",
    "dataset = load_dataset(SOURCE_DATASET, SOURCE_DATASET_VERSION)\n",
    "dataset[\"test\"] = dataset[\"validation\"]\n",
    "del dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52a8dd69-70db-48bb-a421-71436ccd2ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    question = example[\"question\"]\n",
    "    c_0 = example[\"choices\"][0]\n",
    "    c_1 = example[\"choices\"][1]\n",
    "    c_2 = example[\"choices\"][2]\n",
    "    c_3 = example[\"choices\"][3]\n",
    "    c_4 = example[\"choices\"][4]\n",
    "\n",
    "    input = f\"{question}\\nAnswer Choices:\\n(a) {c_0}\\n(b) {c_1}\\n(c) {c_2}\\n(d) {c_3}\\n(e) {c_4}\"\n",
    "\n",
    "    example[\"input\"] = input\n",
    "    example[\"label\"] = example[\"answer\"]\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    prepare_input,\n",
    "    remove_columns=[\n",
    "        \"id\",\n",
    "        \"question\",\n",
    "        \"choices\",\n",
    "        \"answer\",\n",
    "        \"abstractive_explanation\",\n",
    "        \"extractive_explanation\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7033ca8-c148-4468-a8b2-8c2f6a6c45be",
   "metadata": {},
   "source": [
    "In order to reduce the time taken for this tutorial, and to ensure consistent results, the outputs of the PaLM API are pre-generated and provided in JSON format. We will download these outputs and include them in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "738ff8d3-65db-413e-a5d7-d39837349a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://github-repo/distillation/PaLM_CoT_test.json...\n",
      "Copying gs://github-repo/distillation/PaLM_CoT_train.json...                    \n",
      "\\ [2 files][  1.8 MiB/  1.8 MiB]                                                \n",
      "Operation completed over 2 objects/1.8 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp gs://github-repo/distillation/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81c110ef-384f-4b8e-a58a-8c81e3ad6d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100% 1/1 [00:00<00:00, 4604.07it/s]\n",
      "Extracting data files: 100% 1/1 [00:00<00:00, 580.93it/s]\n",
      "Generating train split: 9741 examples [00:00, 428330.31 examples/s]\n",
      "Map: 100% 9741/9741 [00:01<00:00, 5734.98 examples/s]\n",
      "Downloading data files: 100% 1/1 [00:00<00:00, 929.59it/s]\n",
      "Extracting data files: 100% 1/1 [00:00<00:00, 751.53it/s]\n",
      "Generating train split: 1221 examples [00:00, 294273.70 examples/s]\n",
      "Map: 100% 1221/1221 [00:00<00:00, 5118.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "LLM_OUTPUTS_FILE = \"PaLM_CoT_{split}.json\"\n",
    "\n",
    "\n",
    "def add_llm_outputs(dataset: DatasetDict, split: str) -> None:\n",
    "    llm_ds = load_dataset(\"json\", data_files=LLM_OUTPUTS_FILE.format(split=split))[\n",
    "        \"train\"\n",
    "    ]\n",
    "\n",
    "    def _add(example: Dict[str, Any], idx: int) -> Dict[str, Any]:\n",
    "        example[\"llm_rationale\"] = llm_ds[idx][\"rationale\"]\n",
    "        example[\"llm_label\"] = llm_ds[idx][\"label\"]\n",
    "        return example\n",
    "\n",
    "    dataset[split] = dataset[split].map(_add, with_indices=True)\n",
    "\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    add_llm_outputs(dataset, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e8263-1c01-4687-ade0-e85a194e17f6",
   "metadata": {},
   "source": [
    "# Step 2: Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc9306cc-8db2-4715-9ced-8a72d5e47fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a6709-25ba-4036-9735-47c8a105abf6",
   "metadata": {},
   "source": [
    "Here, we will use the T5 model as a pretrained base for distillation, and we will use the corresponding tokenizer. You can use a different pretrained model (and corresponding tokenizer) by changing the name of the model below to a different model on HuggingFace Hub, or use a custom model/train a tokenizer from scratch on your own dataset. Note that you will need significantly more data and compute to train a good model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5da620fa-5e76-47b5-b29f-08eaa239ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_BASE_MODEL = \"google/t5-v1_1-base\"\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_OUTPUT_LENGTH = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880c1dc-c88a-43f7-a25c-a7fcb9c410ae",
   "metadata": {},
   "source": [
    "## a) Prepare the tokenizer and tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af7dceb4-582a-49b6-8fff-9ed709256863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100% 9741/9741 [00:04<00:00, 2383.95 examples/s]\n",
      "Map: 100% 1221/1221 [00:00<00:00, 3136.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BASE_MODEL)\n",
    "\n",
    "\n",
    "def tokenize_function(examples: Dict[str, List[Any]]):\n",
    "    # Encode input to generate predictions and rationales\n",
    "    model_inputs = tokenizer(\n",
    "        [\"predict: \" + text for text in examples[\"input\"]],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    expl_model_inputs = tokenizer(\n",
    "        [\"explain: \" + text for text in examples[\"input\"]],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    model_inputs[\"expl_input_ids\"] = expl_model_inputs[\"input_ids\"]\n",
    "    model_inputs[\"expl_attention_mask\"] = expl_model_inputs[\"attention_mask\"]\n",
    "\n",
    "    # Encode target label and target rationale\n",
    "    label_output_encodings = tokenizer(\n",
    "        text_target=examples[\"label\"], max_length=MAX_OUTPUT_LENGTH, truncation=True\n",
    "    )\n",
    "    rationale_output_encodings = tokenizer(\n",
    "        text_target=examples[\"llm_rationale\"],\n",
    "        max_length=MAX_OUTPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = label_output_encodings[\"input_ids\"]\n",
    "    model_inputs[\"expl_labels\"] = rationale_output_encodings[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=[\"input\", \"llm_rationale\", \"label\", \"llm_label\"],\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb46c2-961b-4c30-a175-bdd36a325a88",
   "metadata": {},
   "source": [
    "## b) Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c3eab9b-ee7d-49da-8875-c14693358e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_BASE_MODEL)\n",
    "# Uncomment if you have more than one GPU to enable parallelism\n",
    "# model.parallelize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9507d2e-6e78-4b38-ac22-208b9b81a791",
   "metadata": {},
   "source": [
    "## c) Prepare data collator for multi-task training\n",
    "Since we need to generate predictions for both the answer as well as the rationale on each training and prediction step, we will use a custom DataCollator which will take each batch of features and return two sets of features and labels, one each for the answer and for the rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6695644-b788-4bab-ba72-cb8a2789ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskPrefixDataCollator(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        features_df = pd.DataFrame(features)\n",
    "\n",
    "        # Generate features for answers\n",
    "        ans_features = features_df.loc[\n",
    "            :, features_df.columns.isin([\"labels\", \"input_ids\", \"attention_mask\"])\n",
    "        ].to_dict(\"records\")\n",
    "        ans_features = super().__call__(ans_features, return_tensors)\n",
    "\n",
    "        # Generate features for explanations\n",
    "        expl_features = (\n",
    "            features_df.loc[\n",
    "                :,\n",
    "                features_df.columns.isin(\n",
    "                    [\"expl_labels\", \"expl_input_ids\", \"expl_attention_mask\"]\n",
    "                ),\n",
    "            ]\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"expl_labels\": \"labels\",\n",
    "                    \"expl_input_ids\": \"input_ids\",\n",
    "                    \"expl_attention_mask\": \"attention_mask\",\n",
    "                }\n",
    "            )\n",
    "            .to_dict(\"records\")\n",
    "        )\n",
    "        expl_features = super().__call__(expl_features, return_tensors)\n",
    "\n",
    "        return {\n",
    "            \"ans\": ans_features,\n",
    "            \"expl\": expl_features,\n",
    "        }\n",
    "\n",
    "\n",
    "data_collator = TaskPrefixDataCollator(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e5c8c-0830-43ad-bcc8-ccdbfc8c3776",
   "metadata": {},
   "source": [
    "## d) Prepare trainer for multi-task training\n",
    "Similarly, we will use a custom Trainer for training the model, which takes into account both the losses for answer generation as well as rationale generation. We will use a hyperparameter `alpha` to control the relative contribution of the two losses to the overall model loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0083b-b494-4dd8-ab58-29f850d2fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskPrefixTrainer(Seq2SeqTrainer):\n",
    "    def __init__(self, alpha, output_rationale, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.output_rationale = output_rationale\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        ans_outputs = model(**inputs[\"ans\"])\n",
    "        expl_outputs = model(**inputs[\"expl\"])\n",
    "\n",
    "        loss = self.alpha * ans_outputs.loss + (1.0 - self.alpha) * expl_outputs.loss\n",
    "\n",
    "        return (\n",
    "            (loss, {\"ans\": ans_outputs, \"expl\": expl_outputs})\n",
    "            if return_outputs\n",
    "            else loss\n",
    "        )\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        ans_outputs = super().prediction_step(\n",
    "            model, inputs[\"ans\"], prediction_loss_only=False, ignore_keys=ignore_keys\n",
    "        )\n",
    "        if self.output_rationale:\n",
    "            expl_outputs = super().prediction_step(\n",
    "                model,\n",
    "                inputs[\"expl\"],\n",
    "                prediction_loss_only=False,\n",
    "                ignore_keys=ignore_keys,\n",
    "            )\n",
    "        else:\n",
    "            expl_outputs = ans_outputs  # placeholder only\n",
    "\n",
    "        loss = self.alpha * ans_outputs[0] + (1 - self.alpha) * expl_outputs[0]\n",
    "\n",
    "        return (\n",
    "            loss,\n",
    "            [ans_outputs[1], expl_outputs[1]],\n",
    "            [ans_outputs[2], expl_outputs[2]],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9019019-957d-4303-9c29-e07908ff6bbe",
   "metadata": {},
   "source": [
    "# Step 3: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82d06883-60c5-4916-b9d7-6ff82e2b40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86b1b2-1ff9-4249-b259-8d80a2c2c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = 0\n",
    "CONFIG_DIR = \"distillation_outputs\"\n",
    "CKPT_DIR = f\"{CONFIG_DIR}/ckpts/{RUN_ID}\"  # for model ckpts\n",
    "LOG_DIR = f\"{CONFIG_DIR}/logs/{RUN_ID}\"  # for training logs\n",
    "\n",
    "EVAL_STEPS = 500\n",
    "SAVE_STEPS = 1000\n",
    "MAX_STEPS = 10000\n",
    "\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "ALPHA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06445292-4027-444b-8a59-cb2ace9e808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(RUN_ID)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    CKPT_DIR,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    logging_dir=LOG_DIR,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=EVAL_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    predict_with_generate=True,\n",
    "    seed=RUN_ID,\n",
    "    local_rank=-1,\n",
    "    bf16=False,\n",
    "    generation_max_length=64,\n",
    "    prediction_loss_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e470d3a-8598-4899-8a45-b5ee92704198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def compute_metrics_text(tokenizer: AutoTokenizer) -> Callable:\n",
    "    def compute_metrics(eval_pred: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "        predictions, labels = eval_pred\n",
    "        decoded_preds = tokenizer.batch_decode(predictions[0], skip_special_tokens=True)\n",
    "\n",
    "        labels = np.where(labels[0] != -100, labels[0], tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        acc = np.mean(np.array(decoded_preds) == np.array(decoded_labels))\n",
    "\n",
    "        return {\"accuracy\": acc}\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "compute_metrics = compute_metrics_text(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ccb217-b010-4481-8607-e98363bb3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_kwargs = {\n",
    "    \"alpha\": ALPHA,\n",
    "    \"output_rationale\": False,\n",
    "    \"model\": model,\n",
    "    \"args\": training_args,\n",
    "    \"train_dataset\": tokenized_dataset[\"train\"],\n",
    "    \"eval_dataset\": {\n",
    "        \"test\": tokenized_dataset[\"test\"],\n",
    "    },\n",
    "    \"data_collator\": data_collator,\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"compute_metrics\": compute_metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687fe1b-40ad-4ae0-bdf2-cd4a50c2154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TaskPrefixTrainer(**trainer_kwargs)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde36bb-55c8-4e8b-88f0-a63493c22503",
   "metadata": {},
   "source": [
    "# Step 4: Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394bb4ab-ce55-43f3-b8a2-ae01afd0f578",
   "metadata": {},
   "source": [
    "Now let's compare the performance of our distilled student model against the PaLM model. We will also try to generate outputs from the base student model to compare the difference that the distilled training method has made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ac9c6-dc0b-4bbc-9403-b9447055761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b54f85d-450a-474a-b005-15f44f594a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = f\"{CKPT_DIR}/checkpoint-9000\"\n",
    "\n",
    "distilled_tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "distilled_model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BASE_MODEL)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eafc3e5-6116-4370-be03-96a88c08fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_generator = pipeline(\n",
    "    \"text2text-generation\", model=distilled_model, tokenizer=distilled_tokenizer\n",
    ")\n",
    "base_generator = pipeline(\n",
    "    \"text2text-generation\", model=base_model, tokenizer=base_tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answers(sample: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    sample[\"distill_label\"] = distill_generator([\"predict: \" + sample[\"input\"]])[0][\n",
    "        \"generated_text\"\n",
    "    ]\n",
    "    sample[\"base_label\"] = base_generator(sample[\"input\"])[0][\"generated_text\"]\n",
    "    return sample\n",
    "\n",
    "\n",
    "output_dataset = dataset[\"test\"].map(generate_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc3a499-0861-42b8-9d30-953d08b32543",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = (\n",
    "    output_dataset.to_pandas()\n",
    "    .drop(\"llm_rationale\", axis=1)\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"input\": \"Question\",\n",
    "            \"label\": \"True answer\",\n",
    "            \"llm_label\": \"PaLM answer\",\n",
    "            \"base_label\": \"T5 answer\",\n",
    "            \"distill_label\": \"Distilled T5 answer\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "output_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05aed0-57ab-4097-aad0-74bef2d7451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The accuracy of PaLM model is {:.2f}%\".format(\n",
    "        output_df[output_df[\"label\"] == output_df[\"llm_label\"]][\"label\"].count()\n",
    "        / len(output_df)\n",
    "        * 100\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The accuracy of raw student model is {:.2f}%\".format(\n",
    "        output_df[output_df[\"label\"] == output_df[\"base_label\"]][\"label\"].count()\n",
    "        / len(output_df)\n",
    "        * 100\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The accuracy of distilled student model is {:.2f}%\".format(\n",
    "        output_df[output_df[\"label\"] == output_df[\"distill_label\"]][\"label\"].count()\n",
    "        / len(output_df)\n",
    "        * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44724e-a62c-4ce1-af69-5bdf268e6aaf",
   "metadata": {},
   "source": [
    "As we can see, the raw pretrained student model is unable to generate answers. However, with just a few training samples and epochs, we are able to approach the accuracy of the PaLM model using the much smaller T5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045970e-89f5-4cb5-8a67-99304c87b07f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distillation (Local)",
   "language": "python",
   "name": "local-distillation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
