{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3710d312-cbbe-4fc1-9db6-712e8bccddd9",
   "metadata": {},
   "source": [
    "# Distillation step-by-step\n",
    "### A step-by-step guide :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c72c65c-b6b5-47ac-915b-f1a19c5cf312",
   "metadata": {},
   "source": [
    "In this notebook, we will go over the technique described in the paper [Distilling step-by-step](https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html), which describes a novel approach to distill the knowledge of a large LLM into a smaller LLM without requiring the internal parameters of the larger model. The original code from the research is available at [https://github.com/google-research/distilling-step-by-step](https://github.com/google-research/distilling-step-by-step).\n",
    "\n",
    "In this notebook, we will go through each step of training a small (student) model to mimic the reasoning ability of a larger (teacher) model. By training the student model to mimic the reasoning ability rather than the actual outputs, we can make the smaller model generalize better to other unseen inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f998a431-4219-4716-a61a-b7ebe48cace5",
   "metadata": {},
   "source": [
    "## Step 1: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2eb53-9454-46ac-b3a4-5b01e93ba78e",
   "metadata": {},
   "source": [
    "Our dataset will need three fields -\n",
    "1. An input prompt for the LLM\n",
    "2. A ground truth label, which is the expected output\n",
    "3. A 'rationale', which is the reasoning generated by the teacher model (using CoT prompting)\n",
    "\n",
    "Here, we will use the [Common Sense Explanations](https://huggingface.co/datasets/cos_e) dataset from HuggingFace to train our student model. This dataset contains around 10k training samples and 1.2k test samples. We will use pre-generated rationales from the PaLM model as a teacher, and we will preprocess the dataset to fit the above schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefb0de-17b8-4f41-b56a-f3afdf21e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe43325-f2c7-4627-a561-1fe7aeb7828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DATASET = \"cos_e\"\n",
    "SOURCE_DATASET_VERSION = \"v1.11\"\n",
    "\n",
    "dataset = load_dataset(SOURCE_DATASET, SOURCE_DATASET_VERSION)\n",
    "dataset[\"test\"] = dataset[\"validation\"]\n",
    "del dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8dd69-70db-48bb-a421-71436ccd2ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(example):\n",
    "    question = example[\"question\"]\n",
    "    c_0 = example[\"choices\"][0]\n",
    "    c_1 = example[\"choices\"][1]\n",
    "    c_2 = example[\"choices\"][2]\n",
    "    c_3 = example[\"choices\"][3]\n",
    "    c_4 = example[\"choices\"][4]\n",
    "\n",
    "    input = f\"{question}\\nAnswer Choices:\\n(a) {c_0}\\n(b) {c_1}\\n(c) {c_2}\\n(d) {c_3}\\n(e) {c_4}\"\n",
    "\n",
    "    example[\"input\"] = input\n",
    "    example[\"label\"] = example[\"answer\"]\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    prepare_input,\n",
    "    remove_columns=[\n",
    "        \"id\",\n",
    "        \"question\",\n",
    "        \"choices\",\n",
    "        \"answer\",\n",
    "        \"abstractive_explanation\",\n",
    "        \"extractive_explanation\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c110ef-384f-4b8e-a58a-8c81e3ad6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_OUTPUTS_FILE = \"PaLM_CoT_{split}.json\"\n",
    "\n",
    "\n",
    "def add_llm_outputs(dataset, split):\n",
    "    llm_ds = load_dataset(\"json\", data_files=LLM_OUTPUTS_FILE.format(split=split))[\n",
    "        \"train\"\n",
    "    ]\n",
    "\n",
    "    def _add(example, idx):\n",
    "        example[\"llm_rationale\"] = llm_ds[idx][\"rationale\"]\n",
    "        example[\"llm_label\"] = llm_ds[idx][\"label\"]\n",
    "        return example\n",
    "\n",
    "    dataset[split] = dataset[split].map(_add, with_indices=True)\n",
    "\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    add_llm_outputs(dataset, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e8263-1c01-4687-ade0-e85a194e17f6",
   "metadata": {},
   "source": [
    "## Step 2: Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9306cc-8db2-4715-9ced-8a72d5e47fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a6709-25ba-4036-9735-47c8a105abf6",
   "metadata": {},
   "source": [
    "Here, we will use the T5 model as a pretrained base for distillation, and we will use the corresponding tokenizer. You can use a different pretrained model (and corresponding tokenizer) by changing the name of the model below to a different model on HuggingFace Hub, or use a custom model/train a tokenizer from scratch on your own dataset. Note that you will need significantly more data and compute to train a good model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da620fa-5e76-47b5-b29f-08eaa239ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_BASE_MODEL = \"google/t5-v1_1-base\"\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_OUTPUT_LENGTH = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880c1dc-c88a-43f7-a25c-a7fcb9c410ae",
   "metadata": {},
   "source": [
    "### a) Prepare the tokenizer and tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7dceb4-582a-49b6-8fff-9ed709256863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BASE_MODEL)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Encode input to generate predictions and rationales\n",
    "    model_inputs = tokenizer(\n",
    "        [\"predict: \" + text for text in examples[\"input\"]],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    expl_model_inputs = tokenizer(\n",
    "        [\"explain: \" + text for text in examples[\"input\"]],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    model_inputs[\"expl_input_ids\"] = expl_model_inputs[\"input_ids\"]\n",
    "    model_inputs[\"expl_attention_mask\"] = expl_model_inputs[\"attention_mask\"]\n",
    "\n",
    "    # Encode target label and target rationale\n",
    "    label_output_encodings = tokenizer(\n",
    "        text_target=examples[\"label\"], max_length=MAX_OUTPUT_LENGTH, truncation=True\n",
    "    )\n",
    "    rationale_output_encodings = tokenizer(\n",
    "        text_target=examples[\"llm_rationale\"],\n",
    "        max_length=MAX_OUTPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = label_output_encodings[\"input_ids\"]\n",
    "    model_inputs[\"expl_labels\"] = rationale_output_encodings[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=[\"input\", \"llm_rationale\", \"label\", \"llm_label\"],\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb46c2-961b-4c30-a175-bdd36a325a88",
   "metadata": {},
   "source": [
    "### b) Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3eab9b-ee7d-49da-8875-c14693358e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_BASE_MODEL)\n",
    "# Uncomment if you have more than one GPU to enable parallelism\n",
    "# model.parallelize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9507d2e-6e78-4b38-ac22-208b9b81a791",
   "metadata": {},
   "source": [
    "### c) Prepare data collator for multi-task training\n",
    "Since we need to generate predictions for both the answer as well as the rationale on each training and prediction step, we will use a custom DataCollator which will take each batch of features and return two sets of features and labels, one each for the answer and for the rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6695644-b788-4bab-ba72-cb8a2789ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskPrefixDataCollator(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        features_df = pd.DataFrame(features)\n",
    "\n",
    "        # Generate features for answers\n",
    "        ans_features = features_df.loc[\n",
    "            :, features_df.columns.isin([\"labels\", \"input_ids\", \"attention_mask\"])\n",
    "        ].to_dict(\"records\")\n",
    "        ans_features = super().__call__(ans_features, return_tensors)\n",
    "\n",
    "        # Generate features for explanations\n",
    "        expl_features = (\n",
    "            features_df.loc[\n",
    "                :,\n",
    "                features_df.columns.isin(\n",
    "                    [\"expl_labels\", \"expl_input_ids\", \"expl_attention_mask\"]\n",
    "                ),\n",
    "            ]\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"expl_labels\": \"labels\",\n",
    "                    \"expl_input_ids\": \"input_ids\",\n",
    "                    \"expl_attention_mask\": \"attention_mask\",\n",
    "                }\n",
    "            )\n",
    "            .to_dict(\"records\")\n",
    "        )\n",
    "        expl_features = super().__call__(expl_features, return_tensors)\n",
    "\n",
    "        return {\n",
    "            \"ans\": ans_features,\n",
    "            \"expl\": expl_features,\n",
    "        }\n",
    "\n",
    "\n",
    "data_collator = TaskPrefixDataCollator(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e5c8c-0830-43ad-bcc8-ccdbfc8c3776",
   "metadata": {},
   "source": [
    "### d) Prepare trainer for multi-task training\n",
    "Similarly, we will use a custom Trainer for training the model, which takes into account both the losses for answer generation as well as rationale generation. We will use a hyperparameter `alpha` to control the relative contribution of the two losses to the overall model loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0083b-b494-4dd8-ab58-29f850d2fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskPrefixTrainer(Seq2SeqTrainer):\n",
    "    def __init__(self, alpha, output_rationale, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.output_rationale = output_rationale\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        ans_outputs = model(**inputs[\"ans\"])\n",
    "        expl_outputs = model(**inputs[\"expl\"])\n",
    "\n",
    "        loss = self.alpha * ans_outputs.loss + (1.0 - self.alpha) * expl_outputs.loss\n",
    "\n",
    "        return (\n",
    "            (loss, {\"ans\": ans_outputs, \"expl\": expl_outputs})\n",
    "            if return_outputs\n",
    "            else loss\n",
    "        )\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        ans_outputs = super().prediction_step(\n",
    "            model, inputs[\"ans\"], prediction_loss_only=False, ignore_keys=ignore_keys\n",
    "        )\n",
    "        if self.output_rationale:\n",
    "            expl_outputs = super().prediction_step(\n",
    "                model,\n",
    "                inputs[\"expl\"],\n",
    "                prediction_loss_only=False,\n",
    "                ignore_keys=ignore_keys,\n",
    "            )\n",
    "        else:\n",
    "            expl_outputs = ans_outputs  # placeholder only\n",
    "\n",
    "        loss = self.alpha * ans_outputs[0] + (1 - self.alpha) * expl_outputs[0]\n",
    "\n",
    "        return (\n",
    "            loss,\n",
    "            [ans_outputs[1], expl_outputs[1]],\n",
    "            [ans_outputs[2], expl_outputs[2]],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9019019-957d-4303-9c29-e07908ff6bbe",
   "metadata": {},
   "source": [
    "## Step 3: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d06883-60c5-4916-b9d7-6ff82e2b40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86b1b2-1ff9-4249-b259-8d80a2c2c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = 0\n",
    "CONFIG_DIR = \"distillation_outputs\"\n",
    "CKPT_DIR = f\"{CONFIG_DIR}/ckpts/{RUN_ID}\"  # for model ckpts\n",
    "LOG_DIR = f\"{CONFIG_DIR}/logs/{RUN_ID}\"  # for training logs\n",
    "\n",
    "EVAL_STEPS = 500\n",
    "SAVE_STEPS = 1000\n",
    "MAX_STEPS = 10000\n",
    "\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "ALPHA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06445292-4027-444b-8a59-cb2ace9e808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(RUN_ID)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    CKPT_DIR,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    logging_dir=LOG_DIR,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=EVAL_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    predict_with_generate=True,\n",
    "    seed=RUN_ID,\n",
    "    local_rank=-1,\n",
    "    bf16=False,\n",
    "    generation_max_length=64,\n",
    "    prediction_loss_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e470d3a-8598-4899-8a45-b5ee92704198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_text(tokenizer):\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        decoded_preds = tokenizer.batch_decode(predictions[0], skip_special_tokens=True)\n",
    "\n",
    "        labels = np.where(labels[0] != -100, labels[0], tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        acc = np.mean(np.array(decoded_preds) == np.array(decoded_labels))\n",
    "\n",
    "        return {\"accuracy\": acc}\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "compute_metrics = compute_metrics_text(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ccb217-b010-4481-8607-e98363bb3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_kwargs = {\n",
    "    \"alpha\": ALPHA,\n",
    "    \"output_rationale\": False,\n",
    "    \"model\": model,\n",
    "    \"args\": training_args,\n",
    "    \"train_dataset\": tokenized_dataset[\"train\"],\n",
    "    \"eval_dataset\": {\n",
    "        \"test\": tokenized_dataset[\"test\"],\n",
    "    },\n",
    "    \"data_collator\": data_collator,\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"compute_metrics\": compute_metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687fe1b-40ad-4ae0-bdf2-cd4a50c2154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TaskPrefixTrainer(**trainer_kwargs)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde36bb-55c8-4e8b-88f0-a63493c22503",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394bb4ab-ce55-43f3-b8a2-ae01afd0f578",
   "metadata": {},
   "source": [
    "Now let's compare the performance of our distilled student model against the PaLM model. We will also try to generate outputs from the base student model to compare the difference that the distilled training method has made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ac9c6-dc0b-4bbc-9403-b9447055761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b54f85d-450a-474a-b005-15f44f594a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = f\"{CKPT_DIR}/checkpoint-9000\"\n",
    "\n",
    "distilled_tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "distilled_model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BASE_MODEL)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eafc3e5-6116-4370-be03-96a88c08fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_generator = pipeline(\"text2text-generation\", model=distilled_model, tokenizer=distilled_tokenizer)\n",
    "base_generator = pipeline(\"text2text-generation\", model=base_model, tokenizer=base_tokenizer)\n",
    "\n",
    "def generate_answers(sample):\n",
    "    sample[\"distill_label\"] = distill_generator([\"predict: \" + sample[\"input\"]])[0][\"generated_text\"]\n",
    "    sample[\"base_label\"] = base_generator(sample[\"input\"])[0][\"generated_text\"]\n",
    "    return sample\n",
    "    \n",
    "output_dataset = dataset[\"test\"].map(generate_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc3a499-0861-42b8-9d30-953d08b32543",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = output_dataset.to_pandas().drop(\"llm_rationale\", axis=1).rename(columns={\n",
    "    \"input\": \"Question\",\n",
    "    \"label\": \"True answer\",\n",
    "    \"llm_label\": \"PaLM answer\",\n",
    "    \"base_label\": \"T5 answer\",\n",
    "    \"distill_label\": \"Distilled T5 answer\"\n",
    "})\n",
    "output_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05aed0-57ab-4097-aad0-74bef2d7451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy of PaLM model is {:.2f}%\".format(output_df[output_df[\"label\"] == output_df[\"llm_label\"]][\"label\"].count()/len(output_df)*100))\n",
    "print(\"The accuracy of raw student model is {:.2f}%\".format(output_df[output_df[\"label\"] == output_df[\"base_label\"]][\"label\"].count()/len(output_df)*100))\n",
    "print(\"The accuracy of distilled student model is {:.2f}%\".format(output_df[output_df[\"label\"] == output_df[\"distill_label\"]][\"label\"].count()/len(output_df)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distillation (Local)",
   "language": "python",
   "name": "local-distillation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
