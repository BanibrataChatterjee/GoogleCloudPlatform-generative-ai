{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Generative App Using Palm 2 & LangChain"
      ],
      "metadata": {
        "id": "Uj7R48-ZXmTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation & Authentication\n",
        "\n",
        "**Install google-generativeai & langchain**\n",
        "- Get API KEY from MakerSuite or Google Cloud"
      ],
      "metadata": {
        "id": "SPG8eRQCcNQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY='APIKey'"
      ],
      "metadata": {
        "id": "6zYFa0au2Ou4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain google-generativeai"
      ],
      "metadata": {
        "id": "IdsE3DEJcM35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Or) Authenticate & Initialize Google Cloud Project**"
      ],
      "metadata": {
        "id": "ptwMB9pqcniz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "    google_auth.authenticate_user()\n",
        "\n",
        "PROJECT_ID = \"api-project-503433767370\"  # @param {type:\"string\"}\n",
        "\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "HP80SWi0rIBL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Google Cloud AI Platform and Define Vertex AI Base Model & Class**"
      ],
      "metadata": {
        "id": "4lcArn48r6pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Vertex AI LLM SDK# Install Vertex AI LLM SDK\n",
        "! pip install google-cloud-aiplatform==1.25.0\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "\n",
        "from pydantic import BaseModel, root_validator\n",
        "from typing import Any, Mapping, Optional, List, Dict\n",
        "from langchain.llms.base import LLM\n",
        "\n",
        "class _VertexCommon(BaseModel):\n",
        "    \"\"\"Wrapper around Vertex AI large language models.\n",
        "\n",
        "    To use, you should have the\n",
        "    ``google.cloud.aiplatform.private_preview.language_models`` python package\n",
        "    installed.\n",
        "    \"\"\"\n",
        "    client: Any = None #: :meta private:\n",
        "    model_name: str = \"text-bison@001\"\n",
        "    \"\"\"Model name to use.\"\"\"\n",
        "\n",
        "    temperature: float = 0.2\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    top_p: int = 0.8\n",
        "    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n",
        "\n",
        "    top_k: int = 40\n",
        "    \"\"\"The number of highest probability tokens to keep for top-k filtering.\"\"\"\n",
        "\n",
        "    max_output_tokens: int = 200\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def _default_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the default parameters for calling Vertex AI API.\"\"\"\n",
        "        return {\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"max_output_tokens\": self.max_output_tokens\n",
        "        }\n",
        "\n",
        "    def _predict(self, prompt: str, stop: Optional[List[str]]) -> str:\n",
        "        res = self.client.predict(prompt, **self._default_params)\n",
        "        return self._enforce_stop_words(res.text, stop)\n",
        "\n",
        "    def _enforce_stop_words(self, text: str, stop: Optional[List[str]]) -> str:\n",
        "        if stop:\n",
        "            return enforce_stop_tokens(text, stop)\n",
        "        return text\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of llm.\"\"\"\n",
        "        return \"vertex_ai\"\n",
        "\n",
        "class VertexLLM(_VertexCommon, LLM):\n",
        "    model_name: str = \"text-bison@001\"\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
        "        try:\n",
        "            from vertexai.preview.language_models import TextGenerationModel\n",
        "        except ImportError:\n",
        "            raise ValueError(\n",
        "                \"Could not import Vertex AI LLM python package. \"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            values[\"client\"] = TextGenerationModel.from_pretrained(values[\"model_name\"])\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"Could not set Vertex Text Model client.\"\n",
        "            )\n",
        "\n",
        "        return values\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Call out to Vertex AI's create endpoint.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt to pass into the model.\n",
        "\n",
        "        Returns:\n",
        "            The string generated by the model.\n",
        "        \"\"\"\n",
        "        return self._predict(prompt, stop)\n",
        "\n"
      ],
      "metadata": {
        "id": "ax6hlCt7YbXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiatlize Vertex AI"
      ],
      "metadata": {
        "id": "7yHf4ipxYyfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Vertex AI SDK\n",
        "import vertexai\n",
        "PROJECT_ID = \"api-project-503433767370\"  # @param {type:\"string\"}\n",
        "\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "EZe8iS2CY2E8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Invoking Chat Bison - Raw"
      ],
      "metadata": {
        "id": "PmE0xEvkl9MQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Invoke Chat Bison*"
      ],
      "metadata": {
        "id": "gTkOBnHyfxVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import google.generativeai as palm\n",
        "\n",
        "palm.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "defaults = {\n",
        "  'model': 'models/chat-bison-001',\n",
        "  'temperature': 0.25,\n",
        "  'candidate_count': 1,\n",
        "  'top_k': 40,\n",
        "  'top_p': 0.95,\n",
        "}\n",
        "context = \"You are an astronomer, knowledgeable about the solar system.\"\n",
        "examples = [\n",
        "  [\n",
        "    \"What languages do you know\",\n",
        "    \"I can speak and translate between over 200 languages\"\n",
        "  ],\n",
        "  [\n",
        "    \"Write a  poem in English\",\n",
        "    \"Sure, here is a poem I wrote in English:\\n\\nThe world is a beautiful place\"\n",
        "  ]\n",
        "]\n",
        "messages = [\n",
        "  \"Write a poem on love in english\",\n",
        "  \"**Love's Embrace**\\n\\nLove is a mystery, a force so deep\"\n",
        "]\n",
        "messages.append(\"How many moons does Mars have?\")\n",
        "response = palm.chat(\n",
        "  **defaults,\n",
        "  context=context,\n",
        "  examples=examples,\n",
        "  messages=messages\n",
        ")\n",
        "print(response.last) # Response of the AI to your most recent request"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkKnALpQzQm2",
        "outputId": "2a65d1d5-02d7-4ac1-f60d-d6fbcbfd9785"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mars has two moons, Phobos and Deimos. Phobos is the larger of the two moons, and is about 22 kilometers in diameter. Deimos is the smaller moon, and is about 12 kilometers in diameter. Both moons are thought to be captured asteroids, and are very irregularly shaped. They are also very close to Mars, with orbits that take only a few hours to complete. This means that they appear to move very quickly across the Martian sky.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Invoking Chat Bison Using LangChain Prompt Template"
      ],
      "metadata": {
        "id": "19iifnZ5luA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Building a Prompt Template with LangChain*"
      ],
      "metadata": {
        "id": "blSr3Ihfdw7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are a helpful assistant that re-writes the user's text to \"\n",
        "                \"sound more upbeat.\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
        "    ]\n",
        ")\n",
        "print(template.format_messages(text='i dont like eating tasty things.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4_GErH3duKd",
        "outputId": "de269b14-6205-4ecb-ece7-b3789099bb67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content='i dont like eating tasty things.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoking Text Bison directly"
      ],
      "metadata": {
        "id": "-EeiGb_cnThu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = VertexLLM(\n",
        "    model_name='text-bison-32k',\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "\n",
        "llm('i dont like eating tasty things.')\n",
        "#llm(template.format_messages(text='i dont like eating tasty things.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "hloE0JOahc3Q",
        "outputId": "8f69c5d3-0cbd-4a60-d783-1c48fda9bb80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' There are many reasons why someone might not like eating tasty things. Some people may have a medical condition that makes it difficult or painful to eat, while others may simply not enjoy the taste of food. Still others may have a psychological aversion to food, or may have had a negative experience with food in the past. If you are concerned about your lack of appetite, it is important to talk to your doctor to rule out any underlying medical conditions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}
