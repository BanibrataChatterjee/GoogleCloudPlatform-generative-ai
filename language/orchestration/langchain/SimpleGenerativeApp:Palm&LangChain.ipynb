{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Generative App Using Palm 2 & LangChain"
      ],
      "metadata": {
        "id": "Uj7R48-ZXmTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation & Authentication\n",
        "\n",
        "**Install google-generativeai & langchain**\n",
        "- Get API KEY from MakerSuite or Google Cloud"
      ],
      "metadata": {
        "id": "SPG8eRQCcNQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY='APIKey'"
      ],
      "metadata": {
        "id": "6zYFa0au2Ou4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain google-generativeai"
      ],
      "metadata": {
        "id": "IdsE3DEJcM35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Or) Authenticate & Initialize Google Cloud Project**"
      ],
      "metadata": {
        "id": "ptwMB9pqcniz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "    google_auth.authenticate_user()\n",
        "\n",
        "PROJECT_ID = \"api-project-503433767370\"  # @param {type:\"string\"}\n",
        "\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "HP80SWi0rIBL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Google Cloud AI Platform and Define Vertex AI Base Model & Class**"
      ],
      "metadata": {
        "id": "4lcArn48r6pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Vertex AI LLM SDK# Install Vertex AI LLM SDK\n",
        "! pip install google-cloud-aiplatform==1.25.0\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "\n",
        "from pydantic import BaseModel, root_validator\n",
        "from typing import Any, Mapping, Optional, List, Dict\n",
        "from langchain.llms.base import LLM\n",
        "\n",
        "class _VertexCommon(BaseModel):\n",
        "    \"\"\"Wrapper around Vertex AI large language models.\n",
        "\n",
        "    To use, you should have the\n",
        "    ``google.cloud.aiplatform.private_preview.language_models`` python package\n",
        "    installed.\n",
        "    \"\"\"\n",
        "    client: Any = None #: :meta private:\n",
        "    model_name: str = \"text-bison@001\"\n",
        "    \"\"\"Model name to use.\"\"\"\n",
        "\n",
        "    temperature: float = 0.2\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    top_p: int = 0.8\n",
        "    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n",
        "\n",
        "    top_k: int = 40\n",
        "    \"\"\"The number of highest probability tokens to keep for top-k filtering.\"\"\"\n",
        "\n",
        "    max_output_tokens: int = 200\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def _default_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the default parameters for calling Vertex AI API.\"\"\"\n",
        "        return {\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"max_output_tokens\": self.max_output_tokens\n",
        "        }\n",
        "\n",
        "    def _predict(self, prompt: str, stop: Optional[List[str]]) -> str:\n",
        "        res = self.client.predict(prompt, **self._default_params)\n",
        "        return self._enforce_stop_words(res.text, stop)\n",
        "\n",
        "    def _enforce_stop_words(self, text: str, stop: Optional[List[str]]) -> str:\n",
        "        if stop:\n",
        "            return enforce_stop_tokens(text, stop)\n",
        "        return text\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of llm.\"\"\"\n",
        "        return \"vertex_ai\"\n",
        "\n",
        "class VertexLLM(_VertexCommon, LLM):\n",
        "    model_name: str = \"text-bison@001\"\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
        "        try:\n",
        "            from vertexai.preview.language_models import TextGenerationModel\n",
        "        except ImportError:\n",
        "            raise ValueError(\n",
        "                \"Could not import Vertex AI LLM python package. \"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            values[\"client\"] = TextGenerationModel.from_pretrained(values[\"model_name\"])\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"Could not set Vertex Text Model client.\"\n",
        "            )\n",
        "\n",
        "        return values\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Call out to Vertex AI's create endpoint.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt to pass into the model.\n",
        "\n",
        "        Returns:\n",
        "            The string generated by the model.\n",
        "        \"\"\"\n",
        "        return self._predict(prompt, stop)\n",
        "\n"
      ],
      "metadata": {
        "id": "ax6hlCt7YbXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiatlize Vertex AI"
      ],
      "metadata": {
        "id": "7yHf4ipxYyfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Vertex AI SDK\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "EZe8iS2CY2E8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation & Helper Functions"
      ],
      "metadata": {
        "id": "g0lQvAFylmMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Get the access token.\n",
        "gcloud_token = !gcloud auth print-access-token\n",
        "\n",
        "# Get the token info.\n",
        "gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "\n",
        "# Print the user ID.\n",
        "print(gcloud_tokeninfo['sub'])\n",
        "# Print the user email.\n",
        "print(gcloud_tokeninfo['email'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hltXdJdesiDG",
        "outputId": "adc299c8-ea17-41b7-fb22-a2c31fb013ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "110961358251290988711\n",
            "guruprakash.cr@gmail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import chat_models\n",
        "from langchain.chat_models import ChatGooglePalm\n",
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "\n",
        "#print(\"Help of chat_models is:\", help(ChatGooglePalm))\n",
        "print(\"Dir of chat_models is:\", dir(chat_models))\n",
        "print(\"ID of chat_models is:\", id(chat_models))\n",
        "print(\"Type of chat_models is:\", type(chat_models))\n",
        "print(\"Dir of ChatGooglePalm is:\", dir(ChatGooglePalm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQDbjuHltMjG",
        "outputId": "a544d20c-a5ba-4f8f-fa6b-8dfc0f91d2b1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dir of chat_models is: ['AzureChatOpenAI', 'BedrockChat', 'ChatAnthropic', 'ChatAnyscale', 'ChatCohere', 'ChatFireworks', 'ChatGooglePalm', 'ChatJavelinAIGateway', 'ChatKonko', 'ChatLiteLLM', 'ChatMLflowAIGateway', 'ChatOllama', 'ChatOpenAI', 'ChatVertexAI', 'ErnieBotChat', 'FakeListChatModel', 'HumanInputChatModel', 'JinaChat', 'MiniMaxChat', 'PromptLayerChatOpenAI', 'QianfanChatEndpoint', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'anthropic', 'anyscale', 'azure_openai', 'baidu_qianfan_endpoint', 'base', 'bedrock', 'cohere', 'ernie', 'fake', 'fireworks', 'google_palm', 'human', 'javelin_ai_gateway', 'jinachat', 'konko', 'litellm', 'minimax', 'mlflow_ai_gateway', 'ollama', 'openai', 'promptlayer_openai', 'vertexai']\n",
            "ID of chat_models is: 133061509844000\n",
            "Type of chat_models is: <class 'module'>\n",
            "Dir of ChatGooglePalm is: ['Config', 'InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__call__', '__class__', '__class_getitem__', '__class_vars__', '__config__', '__custom_root_type__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__exclude_fields__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_validators__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__include_fields__', '__init__', '__init_subclass__', '__iter__', '__json_encoder__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__post_root_validators__', '__pre_root_validators__', '__pretty__', '__private_attributes__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__ror__', '__schema_cache__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__try_update_forward_refs__', '__validators__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_agenerate', '_agenerate_with_cache', '_all_required_field_names', '_astream', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_async', '_call_with_config', '_combine_llm_outputs', '_convert_input', '_copy_and_set_values', '_decompose_class', '_enforce_dict_if_root', '_generate', '_generate_with_cache', '_get_invocation_params', '_get_llm_string', '_get_value', '_identifying_params', '_init_private_attributes', '_is_protocol', '_iter', '_lc_kwargs', '_llm_type', '_stream', '_transform_stream_with_config', 'abatch', 'agenerate', 'agenerate_prompt', 'ainvoke', 'apredict', 'apredict_messages', 'astream', 'astream_log', 'atransform', 'batch', 'bind', 'call_as_llm', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'generate', 'generate_prompt', 'get_lc_namespace', 'get_num_tokens', 'get_num_tokens_from_messages', 'get_token_ids', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'predict', 'predict_messages', 'raise_deprecation', 'schema', 'schema_json', 'stream', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_environment', 'with_config', 'with_fallbacks', 'with_retry']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Invoking Chat Bison Without LangChain Prompt Template"
      ],
      "metadata": {
        "id": "PmE0xEvkl9MQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
        "\n",
        "chat = chat_model.start_chat(\n",
        "        context=\"My name is Miles. You are an astronomer, knowledgeable about the solar system.\",\n",
        "        examples=[\n",
        "            InputOutputTextPair(\n",
        "                input_text=\"How many moons does Mars have?\",\n",
        "                output_text=\"The planet Mars has two moons, Phobos and Deimos.\",\n",
        "            ),\n",
        "        ],\n",
        "    )\n",
        "parameters = {\n",
        "        \"temperature\": 0.2,  # Temperature controls the degree of randomness in token selection.\n",
        "        \"max_output_tokens\": 256,  # Token limit determines the maximum amount of text output.\n",
        "        \"top_p\": 0.95,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
        "        \"top_k\": 40,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
        "}\n",
        "response = chat.send_message(\n",
        "        \"i dont like eating tasty things.\", **parameters\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYmaCUankMjI",
        "outputId": "f3bf6da7-ce3a-4f6e-98bd-86b4510685ab"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: I'm sorry to hear that. Tasty things are one of the best parts of life!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Invoking Chat Bison Using LangChain Prompt Template"
      ],
      "metadata": {
        "id": "19iifnZ5luA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Building a Prompt Template with LangChain*"
      ],
      "metadata": {
        "id": "blSr3Ihfdw7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are a helpful assistant that re-writes the user's text to \"\n",
        "                \"sound more upbeat.\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
        "    ]\n",
        ")\n",
        "print(template.format_messages(text='i dont like eating tasty things.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4_GErH3duKd",
        "outputId": "b1affd92-ea0a-437a-802b-0188317ebd10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content='i dont like eating tasty things.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Invoke Chat Bison*"
      ],
      "metadata": {
        "id": "gTkOBnHyfxVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGooglePalm(google_api_key=GOOGLE_API_KEY, temperature=0.5, top_k=30)\n",
        "llm(template.format_messages(text='i dont like eating tasty things.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD-5ZWN9tI4G",
        "outputId": "5914d51e-95e2-458e-b67b-3859790cd3d7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatMessage(content=\"That's okay! There are many other things to enjoy in life besides food. You could try spending time with friends and family, pursuing your hobbies, or learning something new. There's no need to force yourself to eat something you don't like, and there are plenty of other ways to have a good time.\", role='1')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoking Text Bison directly"
      ],
      "metadata": {
        "id": "-EeiGb_cnThu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = VertexLLM(\n",
        "    model_name='text-bison-32k',\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "\n",
        "llm('i dont like eating tasty things.')\n",
        "#llm(template.format_messages(text='i dont like eating tasty things.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "hloE0JOahc3Q",
        "outputId": "8f69c5d3-0cbd-4a60-d783-1c48fda9bb80"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' There are many reasons why someone might not like eating tasty things. Some people may have a medical condition that makes it difficult or painful to eat, while others may simply not enjoy the taste of food. Still others may have a psychological aversion to food, or may have had a negative experience with food in the past. If you are concerned about your lack of appetite, it is important to talk to your doctor to rule out any underlying medical conditions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}
