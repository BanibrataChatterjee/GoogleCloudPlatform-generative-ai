{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Augmented Generation - Procurement Contract Analyst -  Palm2 & LangChain"
      ],
      "metadata": {
        "id": "Uj7R48-ZXmTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation & Authentication\n",
        "\n",
        "**Install google-generativeai & langchain**\n",
        "- Get API KEY from MakerSuite or Google Cloud"
      ],
      "metadata": {
        "id": "SPG8eRQCcNQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY=''"
      ],
      "metadata": {
        "id": "6zYFa0au2Ou4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install langchain and related libraries\n",
        "!pip install langchain google-generativeai unstructured\n",
        "\n",
        "# Install Vertex AI LLM SDK\n",
        "! pip install google-cloud-aiplatform==1.25.0\n"
      ],
      "metadata": {
        "id": "IdsE3DEJcM35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Or) Authenticate & Initialize Google Cloud Project**"
      ],
      "metadata": {
        "id": "ptwMB9pqcniz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "    google_auth.authenticate_user()\n",
        "\n"
      ],
      "metadata": {
        "id": "HP80SWi0rIBL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Google Cloud AI Platform and Define Vertex AI Base Model & Class**"
      ],
      "metadata": {
        "id": "4lcArn48r6pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "\n",
        "from pydantic import BaseModel, Extra, root_validator\n",
        "from typing import Any, Mapping, Optional, List, Dict\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "class _VertexCommon(BaseModel):\n",
        "    \"\"\"Wrapper around Vertex AI large language models.\n",
        "\n",
        "    To use, you should have the\n",
        "    ``google.cloud.aiplatform.private_preview.language_models`` python package\n",
        "    installed.\n",
        "    \"\"\"\n",
        "    client: Any = None #: :meta private:\n",
        "    model_name: str = \"text-bison@001\"\n",
        "    \"\"\"Model name to use.\"\"\"\n",
        "\n",
        "    temperature: float = 0.2\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    top_p: int = 0.8\n",
        "    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n",
        "\n",
        "    top_k: int = 40\n",
        "    \"\"\"The number of highest probability tokens to keep for top-k filtering.\"\"\"\n",
        "\n",
        "    max_output_tokens: int = 200\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def _default_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the default parameters for calling Vertex AI API.\"\"\"\n",
        "        return {\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"max_output_tokens\": self.max_output_tokens\n",
        "        }\n",
        "\n",
        "    def _predict(self, prompt: str, stop: Optional[List[str]]) -> str:\n",
        "        res = self.client.predict(prompt, **self._default_params)\n",
        "        return self._enforce_stop_words(res.text, stop)\n",
        "\n",
        "    def _enforce_stop_words(self, text: str, stop: Optional[List[str]]) -> str:\n",
        "        if stop:\n",
        "            return enforce_stop_tokens(text, stop)\n",
        "        return text\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of llm.\"\"\"\n",
        "        return \"vertex_ai\"\n",
        "\n",
        "class VertexLLM(_VertexCommon, LLM):\n",
        "    model_name: str = \"text-bison@001\"\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
        "        try:\n",
        "            from vertexai.preview.language_models import TextGenerationModel\n",
        "        except ImportError:\n",
        "            raise ValueError(\n",
        "                \"Could not import Vertex AI LLM python package. \"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            values[\"client\"] = TextGenerationModel.from_pretrained(values[\"model_name\"])\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"Could not set Vertex Text Model client.\"\n",
        "            )\n",
        "\n",
        "        return values\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Call out to Vertex AI's create endpoint.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt to pass into the model.\n",
        "\n",
        "        Returns:\n",
        "            The string generated by the model.\n",
        "        \"\"\"\n",
        "        return self._predict(prompt, stop)\n",
        "\n",
        "\n",
        "class VertexEmbeddings(Embeddings, BaseModel):\n",
        "    \"\"\"Wrapper around Vertex AI large language models embeddings API.\n",
        "\n",
        "    To use, you should have the\n",
        "    ``google.cloud.aiplatform.private_preview.language_models`` python package\n",
        "    installed.\n",
        "    \"\"\"\n",
        "    model_name: str = \"textembedding-gecko@001\"\n",
        "    \"\"\"Model name to use.\"\"\"\n",
        "\n",
        "    model: Any\n",
        "    requests_per_minute: int = 15\n",
        "\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
        "        try:\n",
        "            from vertexai.preview.language_models import TextEmbeddingModel\n",
        "\n",
        "        except ImportError:\n",
        "            raise ValueError(\n",
        "                \"Could not import Vertex AI LLM python package. \"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            values[\"model\"] = TextEmbeddingModel\n",
        "\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"Could not set Vertex Text Model client.\"\n",
        "            )\n",
        "\n",
        "        return values\n",
        "\n",
        "    class Config:\n",
        "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
        "\n",
        "        extra = Extra.forbid\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "      \"\"\"Call Vertex LLM embedding endpoint for embedding docs\n",
        "      Args:\n",
        "          texts: The list of texts to embed.\n",
        "      Returns:\n",
        "          List of embeddings, one for each text.\n",
        "      \"\"\"\n",
        "      self.model = self.model.from_pretrained(self.model_name)\n",
        "\n",
        "      limiter = rate_limit(self.requests_per_minute)\n",
        "      results = []\n",
        "      docs = list(texts)\n",
        "\n",
        "      while docs:\n",
        "        # Working in batches of 2 because the API apparently won't let\n",
        "        # us send more than 2 documents per request to get embeddings.\n",
        "        head, docs = docs[:2], docs[2:]\n",
        "        # print(f'Sending embedding request for: {head!r}')\n",
        "        chunk = self.model.get_embeddings(head)\n",
        "        results.extend(chunk)\n",
        "        next(limiter)\n",
        "\n",
        "      return [r.values for r in results]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "      \"\"\"Call Vertex LLM embedding endpoint for embedding query text.\n",
        "      Args:\n",
        "        text: The text to embed.\n",
        "      Returns:\n",
        "        Embedding for the text.\n",
        "      \"\"\"\n",
        "      single_result = self.embed_documents([text])\n",
        "      return single_result[0]\n",
        "\n",
        "def rate_limit(max_per_minute):\n",
        "  period = 60 / max_per_minute\n",
        "  print('Waiting')\n",
        "  while True:\n",
        "    before = time.time()\n",
        "    yield\n",
        "    after = time.time()\n",
        "    elapsed = after - before\n",
        "    sleep_time = max(0, period - elapsed)\n",
        "    if sleep_time > 0:\n",
        "      print('.', end='')\n",
        "      time.sleep(sleep_time)\n"
      ],
      "metadata": {
        "id": "ax6hlCt7YbXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ecbcd3-83c9-41bf-cd4a-b4bb5cff1498"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI SDK version: 1.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiatlize Vertex AI"
      ],
      "metadata": {
        "id": "7yHf4ipxYyfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"api-project-503433767370\"  # @param {type:\"string\"}\n",
        "\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "EZe8iS2CY2E8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation & Helper Functions"
      ],
      "metadata": {
        "id": "g0lQvAFylmMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Get the access token.\n",
        "gcloud_token = !gcloud auth print-access-token\n",
        "\n",
        "# Get the token info.\n",
        "gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "\n",
        "# Print the user ID.\n",
        "print(gcloud_tokeninfo['sub'])\n",
        "# Print the user email.\n",
        "print(gcloud_tokeninfo['email'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hltXdJdesiDG",
        "outputId": "7036a88a-5f11-40ff-eccf-79cb05c8a499"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "110961358251290988711\n",
            "guruprakash.cr@gmail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import chat_models\n",
        "from langchain.chat_models import ChatGooglePalm\n",
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "\n",
        "#print(\"Help of chat_models is:\", help(ChatGooglePalm))\n",
        "print(\"Dir of chat_models is:\", dir(chat_models))\n",
        "print(\"ID of chat_models is:\", id(chat_models))\n",
        "print(\"Type of chat_models is:\", type(chat_models))\n",
        "print(\"Dir of ChatGooglePalm is:\", dir(ChatGooglePalm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQDbjuHltMjG",
        "outputId": "1c6f0c58-fdb9-48ec-fbb7-a0847ae0ad89"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dir of chat_models is: ['AzureChatOpenAI', 'BedrockChat', 'ChatAnthropic', 'ChatAnyscale', 'ChatCohere', 'ChatFireworks', 'ChatGooglePalm', 'ChatJavelinAIGateway', 'ChatKonko', 'ChatLiteLLM', 'ChatMLflowAIGateway', 'ChatOllama', 'ChatOpenAI', 'ChatVertexAI', 'ErnieBotChat', 'FakeListChatModel', 'HumanInputChatModel', 'JinaChat', 'MiniMaxChat', 'PromptLayerChatOpenAI', 'QianfanChatEndpoint', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'anthropic', 'anyscale', 'azure_openai', 'baidu_qianfan_endpoint', 'base', 'bedrock', 'cohere', 'ernie', 'fake', 'fireworks', 'google_palm', 'human', 'javelin_ai_gateway', 'jinachat', 'konko', 'litellm', 'minimax', 'mlflow_ai_gateway', 'ollama', 'openai', 'promptlayer_openai', 'vertexai']\n",
            "ID of chat_models is: 138387582569024\n",
            "Type of chat_models is: <class 'module'>\n",
            "Dir of ChatGooglePalm is: ['Config', 'InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__call__', '__class__', '__class_getitem__', '__class_vars__', '__config__', '__custom_root_type__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__exclude_fields__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_validators__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__include_fields__', '__init__', '__init_subclass__', '__iter__', '__json_encoder__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__post_root_validators__', '__pre_root_validators__', '__pretty__', '__private_attributes__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__ror__', '__schema_cache__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__try_update_forward_refs__', '__validators__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_agenerate', '_agenerate_with_cache', '_all_required_field_names', '_astream', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_async', '_call_with_config', '_combine_llm_outputs', '_convert_input', '_copy_and_set_values', '_decompose_class', '_enforce_dict_if_root', '_generate', '_generate_with_cache', '_get_invocation_params', '_get_llm_string', '_get_value', '_identifying_params', '_init_private_attributes', '_is_protocol', '_iter', '_lc_kwargs', '_llm_type', '_stream', '_transform_stream_with_config', 'abatch', 'agenerate', 'agenerate_prompt', 'ainvoke', 'apredict', 'apredict_messages', 'astream', 'astream_log', 'atransform', 'batch', 'bind', 'call_as_llm', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'generate', 'generate_prompt', 'get_lc_namespace', 'get_num_tokens', 'get_num_tokens_from_messages', 'get_token_ids', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'predict', 'predict_messages', 'raise_deprecation', 'schema', 'schema_json', 'stream', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_environment', 'with_config', 'with_fallbacks', 'with_retry']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingest the Contracts to build the context for the LLM\n",
        "\n",
        "*Load all the Procurement Contract Documents*"
      ],
      "metadata": {
        "id": "7YozENqoAmm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import GCSDirectoryLoader\n",
        "loader = GCSDirectoryLoader(project_name=PROJECT_ID, bucket=\"contractunderstandingatticusdataset\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "Td4rD2MQtM1O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Split documents into chunks as needed by the token limit of the LLM and let there be an overlap between the chunks*"
      ],
      "metadata": {
        "id": "5Da7_1bpFGpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the documents into chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "print(f\"# of documents = {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E_qzSuMFHKt",
        "outputId": "2d75262a-4a8f-47ae-f67d-e3f50206f5fa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of documents = 2150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structuring the ingested documents in a vector space using a Vector Database"
      ],
      "metadata": {
        "id": "f0htdnYAHonv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Create an embedding vector engine for all the text in the contract documents that have been ingested*"
      ],
      "metadata": {
        "id": "TuwJlBNwIy0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding has a max of 600 requests per minute so we are within limits\n",
        "# https://cloud.google.com/vertex-ai/docs/quotas\n",
        "\n",
        "REQUESTS_PER_MINUTE = 590\n",
        "\n",
        "embedding = VertexEmbeddings(requests_per_minute=REQUESTS_PER_MINUTE)\n",
        "\n",
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nNWJ6XaH1fa",
        "outputId": "3b22df53-1d53-4c7d-ff9f-20180358f13e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VertexEmbeddings(model_name='textembedding-gecko@001', model=<class 'vertexai.language_models._language_models.TextEmbeddingModel'>, requests_per_minute=590)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Create a vector store and store the embeddings in the vector store*"
      ],
      "metadata": {
        "id": "_A6JEaReI96b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store docs in local vectorstore as index\n",
        "!pip install -q chromadb\n",
        "\n",
        "# it may take a while since API is rate limited\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "contracts_vector_db = Chroma.from_documents(docs, embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITiqOdEDJGOn",
        "outputId": "f6fbaf77-97f6-4c03-bdd4-e4d43e10d960"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting\n",
            "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtain handle to the retriever"
      ],
      "metadata": {
        "id": "oVCftNbU-wp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expose index to the retriever\n",
        "retriever = contracts_vector_db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\":2})"
      ],
      "metadata": {
        "id": "Xb2VCn6e-0zp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a Retrieval QA Chain to use retriever"
      ],
      "metadata": {
        "id": "QcwQNvfN_6Rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create chain to answer questions\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = VertexLLM(\n",
        "    model_name='text-bison-32k',\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Uses LLM to synthesize results from the search index.\n",
        "# We use Vertex PaLM Text API for LLM\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True)"
      ],
      "metadata": {
        "id": "ElWUO3fQAMaH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leverage LLM to search from retriever"
      ],
      "metadata": {
        "id": "BHYlgYQhFTQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example:*"
      ],
      "metadata": {
        "id": "tUZDkQOrGb9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who all entered into agreement with Sagebrush?\"\n",
        "result = qa({\"query\": query})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGTS8x7TFoOn",
        "outputId": "c497302f-4306-4cfd-9ff3-05c11cb480a8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting\n",
            "{'query': 'Who all entered into agreement with Sagebrush?', 'result': ' Allison Transmission Holdings, Inc.', 'source_documents': [Document(page_content='Each party cooperated and participated in the drafting and preparation of this Agreement and the documents referred to herein, and any and all drafts relating thereto exchanged among the parties shall be deemed the work product of all of the parties and may not be construed against any party by reason of its drafting or preparation. Accordingly, any rule of law or any legal decision that would require interpretation of any ambiguities in this Agreement against any party that drafted or prepared it is of no application and is hereby expressly waived by each of the parties hereto, and any controversy over interpretations of this Agreement shall be decided without regards to events of drafting or preparation.\\n\\n[Signature Pages Follow]   7\\n\\nIN WITNESS WHEREOF, each of the parties hereto has executed this COOPERATION AGREEMENT or caused the same to be executed by its duly authorized representative as of the date first above written. Allison Transmission Holdings, Inc.', metadata={'source': 'gs://contractunderstandingatticusdataset/ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt'}), Document(page_content='Each party cooperated and participated in the drafting and preparation of this Agreement and the documents referred to herein, and any and all drafts relating thereto exchanged among the parties shall be deemed the work product of all of the parties and may not be construed against any party by reason of its drafting or preparation. Accordingly, any rule of law or any legal decision that would require interpretation of any ambiguities in this Agreement against any party that drafted or prepared it is of no application and is hereby expressly waived by each of the parties hereto, and any controversy over interpretations of this Agreement shall be decided without regards to events of drafting or preparation.\\n\\n[Signature Pages Follow]   7\\n\\nIN WITNESS WHEREOF, each of the parties hereto has executed this COOPERATION AGREEMENT or caused the same to be executed by its duly authorized representative as of the date first above written. Allison Transmission Holdings, Inc.', metadata={'source': 'gs://contractunderstandingatticusdataset/ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt'})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a Front End"
      ],
      "metadata": {
        "id": "FgYa4k1WiolM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "import gradio as gr\n",
        "import markdown\n",
        "\n",
        "def chatbot(inputtext):\n",
        "    result = qa({\"query\": inputtext})\n",
        "\n",
        "    return result['result'], get_public_url(result['source_documents'][0].metadata['source']), result['source_documents'][0].metadata['source']\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "def get_public_url(uri):\n",
        "    \"\"\"Returns the public URL for a file in Google Cloud Storage.\"\"\"\n",
        "    # Split the URI into its components\n",
        "    components = uri.split(\"/\")\n",
        "\n",
        "    # Get the bucket name\n",
        "    bucket_name = components[2]\n",
        "\n",
        "    # Get the file name\n",
        "    file_name = components[3]\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "    return blob.public_url\n",
        "\n",
        "\n",
        "print(\"Launching Gradio\")\n",
        "\n",
        "iface = gr.Interface(fn=chatbot,\n",
        "                     inputs=[gr.Textbox(label=\"Query\")],\n",
        "                     examples=[\"Who are parties to ADMA agreement\", \"What is the agreement between MICOA & Stratton Cheeseman\", \"What is the commission % that Stratton Cheeseman will get from MICOA and how much will they get if MICOA's revenues are $100\"],\n",
        "                     title=\"Contract Analyst\",\n",
        "                     outputs=[gr.Textbox(label=\"Response\"),\n",
        "                              gr.Textbox(label=\"URL\"),\n",
        "                              gr.Textbox(label=\"Cloud Storage URI\")],\n",
        "                     theme=gr.themes.Soft)\n",
        "\n",
        "iface.launch(share=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "CRjBQsXqirbC",
        "outputId": "d665269c-731a-4348-8c95-8e535e8ef9b9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching Gradio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:611: UserWarning: Theme should be a class loaded from gradio.themes\n",
            "  warnings.warn(\"Theme should be a class loaded from gradio.themes\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7871, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}
