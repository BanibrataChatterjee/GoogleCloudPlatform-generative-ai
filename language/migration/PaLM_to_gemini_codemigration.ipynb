{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "id": "ijGzTHJJUCPY"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code migration from PaLM to Gemini**\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/migration/PaLM_to_gemini_codemigration.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/migration/PaLM_to_gemini_codemigration.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/migration/PaLM_to_gemini_codemigration.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ],
   "metadata": {
    "id": "4fRtiUgPZfJj"
   },
   "id": "4fRtiUgPZfJj"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Neelima Reddy](https://github.com/Neelred) |"
   ],
   "metadata": {
    "id": "YtpP_chNbjtf"
   },
   "id": "YtpP_chNbjtf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Overview**\n",
    "\n",
    "\n",
    "This guide shows how to migrate the Vertex AI SDK for Python from using the PaLM API to using the Gemini API. You can generate text, multi-turn conversations (chat), and code with Gemini. After you migrate, check your responses, because the Gemini output might be different from PaLM output. For more information, see the [Introduction to multimodal classes in the Vertex AI SDK](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini).\n",
    "\n",
    "**Differences between PaLM and Gemini**\n",
    "\n",
    "The following are some differences between Gemini and PaLM models:\n",
    "\n",
    "- Their response structures are different. To learn about the Gemini response structure, see the Gemini API model reference response body.\n",
    "\n",
    "- Their safety categories are different. To learn about differences between Gemini and PaLM safety settings, see Key differences between Gemini and other model families.\n",
    "\n",
    "- Gemini can't perform code completion. If you need to create a code completion application, use the `code-gecko` model. For more information, see Codey code completion model.\n",
    "\n",
    "- For code generation, Gemini has a higher recitation block rate.\n",
    "\n",
    "- The confidence score in Codey code generation models that indicates how confident the model is in its response isn't exposed in Gemini.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Cq2PLBip0Ddu"
   },
   "id": "Cq2PLBip0Ddu"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Update PaLM code to use Gemini models**\n",
    "\n",
    "The methods on the GenerativeModel class are mostly the same as the methods on the PaLM classes. For example, use GenerativeModel.start_chat to replace the PaLM equivalent, ChatModel.start_chat. However, because Google Cloud is always improving and updating Gemini, you might run into some differences. For more information, see the Python SDK Reference\n",
    "\n",
    "To migrate from the PaLM API to the Gemini API, the following code modifications are required:\n",
    "\n",
    "- For all PaLM model classes, you use the GenerativeModel class in Gemini.\n",
    "\n",
    "- To use the GenerativeModel class, run the following import statement:\n",
    "\n",
    "  from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "\n",
    "\n",
    "- To generate text in Gemini, use the GenerativeModel.generate_content method instead of the predict method that's used on PaLM models.  \n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "2TeHsFhiWgev"
   },
   "id": "2TeHsFhiWgev"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# **Getting Started**\n",
    "\n",
    "Install Vertex AI SDK and other required packages"
   ],
   "metadata": {
    "id": "N08EftNIWvei"
   },
   "id": "N08EftNIWvei"
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform"
   ],
   "metadata": {
    "id": "Mn48WWTSt4Jt"
   },
   "execution_count": null,
   "outputs": [],
   "id": "Mn48WWTSt4Jt"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Restart runtime\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step."
   ],
   "metadata": {
    "id": "xK6PLyXE3ocb"
   },
   "id": "xK6PLyXE3ocb"
  },
  {
   "cell_type": "code",
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ],
   "metadata": {
    "id": "z47wmKX_7oLm"
   },
   "id": "z47wmKX_7oLm",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
   ],
   "metadata": {
    "id": "nZEcpJ1p7q6H"
   },
   "id": "nZEcpJ1p7q6H"
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ],
   "metadata": {
    "id": "ZrRpT8Ey7til"
   },
   "id": "ZrRpT8Ey7til",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set Google Cloud project information and initialize Vertex AI SDK To get started using Vertex AI, you must have an existing Google Cloud project and enable the Vertex AI API.\n",
    "\n",
    "Learn more about setting up a project and a development environment.\n",
    "\n"
   ],
   "metadata": {
    "id": "kUBxY2Am7wRk"
   },
   "id": "kUBxY2Am7wRk"
  },
  {
   "cell_type": "code",
   "source": [
    "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ],
   "metadata": {
    "id": "M1OebQqA75fK"
   },
   "id": "M1OebQqA75fK",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Usecase : Basic Text generation**\n",
    "\n",
    "The following code samples show the differences between the PaLM API and Gemini API for creating a text generation model."
   ],
   "metadata": {
    "id": "y6ViL-wgBJ4Y"
   },
   "id": "y6ViL-wgBJ4Y"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "CpLu6E6hc8wj"
   },
   "id": "CpLu6E6hc8wj"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Code sample using the PaLM API**"
   ],
   "metadata": {
    "id": "f-84pT3PYqCx"
   },
   "id": "f-84pT3PYqCx"
  },
  {
   "cell_type": "code",
   "source": [
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@002\")\n",
    "\n",
    "response = model.predict(prompt=\"The opposite of hot is\")\n",
    "print(response.text)  #  'cold.'"
   ],
   "metadata": {
    "id": "UBmKgRa69YOF"
   },
   "id": "UBmKgRa69YOF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Code sample using the Gemini API**"
   ],
   "metadata": {
    "id": "yMhyWZyeXotL"
   },
   "id": "yMhyWZyeXotL"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To load a Gemini model, replace the PaLM model class with the GenerativeModel class\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "4nPg7SG8X7-N"
   },
   "id": "4nPg7SG8X7-N"
  },
  {
   "cell_type": "code",
   "source": [
    "from vertexai.generative_models import GenerativeModel"
   ],
   "metadata": {
    "id": "RPheF3yMXGmu"
   },
   "execution_count": null,
   "outputs": [],
   "id": "RPheF3yMXGmu"
  },
  {
   "cell_type": "code",
   "source": [
    "model = GenerativeModel(\"gemini-1.0-pro\")"
   ],
   "metadata": {
    "id": "SKnBYA4iBYFD"
   },
   "id": "SKnBYA4iBYFD",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "To generate text in Gemini, use the 'generate_content' method instead of the 'predict' method that's used in PaLM models."
   ],
   "metadata": {
    "id": "7cEsVrmTYJp8"
   },
   "id": "7cEsVrmTYJp8"
  },
  {
   "cell_type": "code",
   "source": [
    "response = model.generate_content(\"The opposite of hot is\")\n",
    "\n",
    "\n",
    "print(response.text)"
   ],
   "metadata": {
    "id": "rOnkN5utAlVN"
   },
   "id": "rOnkN5utAlVN",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@002\")\n",
    "\n",
    "response = model.predict(prompt=\"The opposite of hot is\")\n",
    "print(response.text)  #  'cold.'"
   ],
   "metadata": {
    "id": "Tg0aEZ3Lc3by"
   },
   "execution_count": null,
   "outputs": [],
   "id": "Tg0aEZ3Lc3by"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Usecase : Text generation with parameters**\n",
    "\n",
    "The following code samples show the differences between the PaLM API and Gemini API for creating a text generation model, with optional parameters."
   ],
   "metadata": {
    "id": "mF8va1xGDnLz"
   },
   "id": "mF8va1xGDnLz"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Code sample using the PaLM API**"
   ],
   "metadata": {
    "id": "hXHMFtTrAn4q"
   },
   "id": "hXHMFtTrAn4q"
  },
  {
   "cell_type": "code",
   "source": [
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@002\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are an expert at solving word problems.\n",
    "\n",
    "Solve the following problem:\n",
    "\n",
    "I have three houses, each with three cats.\n",
    "each cat owns 4 mittens, and a hat. Each mitten was\n",
    "knit from 7m of yarn, each hat from 4m.\n",
    "How much yarn was needed to make all the items?\n",
    "\n",
    "Think about it step by step, and show your work.\n",
    "\"\"\"\n",
    "\n",
    "response = model.predict(\n",
    "    prompt=prompt, temperature=0.1, max_output_tokens=800, top_p=1.0, top_k=40\n",
    ")\n",
    "\n",
    "print(response.text)"
   ],
   "metadata": {
    "id": "naYr5KIwA1Tm"
   },
   "id": "naYr5KIwA1Tm",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Code sample using the Gemini API**"
   ],
   "metadata": {
    "id": "pZTywU9W_V0P"
   },
   "id": "pZTywU9W_V0P"
  },
  {
   "cell_type": "code",
   "source": [
    "from vertexai.generative_models import GenerativeModel"
   ],
   "metadata": {
    "id": "GhHPGuJN_s4N"
   },
   "execution_count": null,
   "outputs": [],
   "id": "GhHPGuJN_s4N"
  },
  {
   "cell_type": "code",
   "source": [
    "model = GenerativeModel(\"gemini-1.0-pro\")"
   ],
   "metadata": {
    "id": "3kB7vLz5_w8J"
   },
   "execution_count": null,
   "outputs": [],
   "id": "3kB7vLz5_w8J"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To generate text in Gemini, use the generate_content method instead of the predict method that's used on PaLM models."
   ],
   "metadata": {
    "id": "_P_nwwC4_0rV"
   },
   "id": "_P_nwwC4_0rV"
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert at solving word problems.\n",
    "\n",
    "Solve the following problem:\n",
    "\n",
    "I have three houses, each with three cats.\n",
    "each cat owns 4 mittens, and a hat. Each mitten was\n",
    "knit from 7m of yarn, each hat from 4m.\n",
    "How much yarn was needed to make all the items?\n",
    "\n",
    "Think about it step by step, and show your work.\n",
    "\"\"\"\n",
    "\n",
    "response1 = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_output_tokens\": 800,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 40,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "print(response1.text)"
   ],
   "metadata": {
    "id": "v0JJBiymDo19"
   },
   "id": "v0JJBiymDo19",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Usecase : Chat**\n",
    "\n",
    "The following code samples show the differences between the PaLM API and Gemini API for creating a chat model."
   ],
   "metadata": {
    "id": "zq13JmRkEmYb"
   },
   "id": "zq13JmRkEmYb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Code sample using the PaLM API**"
   ],
   "metadata": {
    "id": "Jysr2zCXD7av"
   },
   "id": "Jysr2zCXD7av"
  },
  {
   "cell_type": "code",
   "source": [
    "from vertexai.language_models import ChatModel\n",
    "\n",
    "model = ChatModel.from_pretrained(\"chat-bison@002\")\n",
    "\n",
    "chat = model.start_chat()\n",
    "\n",
    "response = chat.send_message(\n",
    "    \"\"\"\n",
    "Hello! Can you write a 300 word abstract in paragraph format for a research paper I need to write about the impact of AI on society?\n",
    "\"\"\"\n",
    ")\n",
    "print(response.text)\n",
    "\n",
    "response = chat.send_message(\n",
    "    \"\"\"\n",
    "Could you give me a catchy title for the paper?\n",
    "\"\"\"\n",
    ")\n",
    "print(response.text)"
   ],
   "metadata": {
    "id": "ksLUPHzqD-Tr"
   },
   "id": "ksLUPHzqD-Tr",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Code sample using the Gemini API**"
   ],
   "metadata": {
    "id": "hbZowHeoBYzA"
   },
   "id": "hbZowHeoBYzA"
  },
  {
   "cell_type": "code",
   "source": [
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "\n",
    "chat = model.start_chat()\n",
    "\n",
    "\n",
    "response = chat.send_message(\n",
    "    \"\"\"\n",
    "Hello! Can you write a 300 word abstract for a research paper I need to write about the impact of AI on society?\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# for response in responses:\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "\n",
    "response = chat.send_message(\n",
    "    \"\"\"\n",
    "Could you give me a catchy title for the paper?\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# for response in responses:\n",
    "\n",
    "print(response.text)"
   ],
   "metadata": {
    "id": "zux3IYXmEqA_"
   },
   "id": "zux3IYXmEqA_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Usecase : Code generation**\n",
    "\n",
    "The following code samples show the differences between the PaLM API and Gemini API for generating a function that predicts if a year is a leap year."
   ],
   "metadata": {
    "id": "f6qeCb6qFueK"
   },
   "id": "f6qeCb6qFueK"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Code sample using the PaLM API**"
   ],
   "metadata": {
    "id": "RzpHqi4SF-ZG"
   },
   "id": "RzpHqi4SF-ZG"
  },
  {
   "cell_type": "code",
   "source": [
    "from vertexai.language_models import CodeGenerationModel\n",
    "\n",
    "model = CodeGenerationModel.from_pretrained(\"code-bison@002\")\n",
    "\n",
    "response = model.predict(\n",
    "    prefix=\"Write a function that checks if a year is a leap year.\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ],
   "metadata": {
    "id": "x3vUJdhKGC-0"
   },
   "id": "x3vUJdhKGC-0",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Code sample using the Gemini API**"
   ],
   "metadata": {
    "id": "kbeza_iHFvpW"
   },
   "id": "kbeza_iHFvpW"
  },
  {
   "cell_type": "code",
   "source": [
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.0-pro-002\")\n",
    "\n",
    "response = model.generate_content(\n",
    "    \"Write a function that checks if a year is a leap year.\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ],
   "metadata": {
    "id": "cpgoxO0dFyQc"
   },
   "id": "cpgoxO0dFyQc",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "colab": {
   "provenance": [],
   "name": "PaLM_to_gemini_codemigration.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}