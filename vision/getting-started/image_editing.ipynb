{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxCkB_DXTHzf"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hny4I-ODTIS6"
      },
      "source": [
        "# Editing with Imagen 2 on Vertex AI\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/image_editing.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fvision%2Fgetting-started%2Fimage_editing.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/vision/getting-started/image_editing.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/image_editing.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Jorj Ismailyan](https://github.com/jismailyan-google), Shuai Tang |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nLS57E2TO5y"
      },
      "source": [
        "## Overview\n",
        "\n",
        "[Imagen 2 on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview) brings Google's state of the art generative AI capabilities to application developers. With Imagen 2 on Vertex AI, application developers can build next-generation AI products that edit images.\n",
        "\n",
        "With Imagen 2, you can not only generate an image, but edit an image using a mask you provide or by setting a mask mode in the request.\n",
        "\n",
        "This notebook focuses on **image editing** only. Learn more about [editing with the Imagen](https://cloud.google.com/vertex-ai/generative-ai/docs/image/edit-images).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXsvgIuwTPZw"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "In this notebook, you will be exploring the image editing features of Imagen using the Vertex AI Python SDK. You will\n",
        "\n",
        "- Edit an image with a text prompt and mask mode.\n",
        "- Insert an object into an image.\n",
        "- Remove an object from an image.\n",
        "- Pad and outpaint an image.\n",
        "- Generate new backgrounds for product images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skXAu__iqks_"
      },
      "source": [
        "### Costs\n",
        "\n",
        "- This notebook uses billable components of Google Cloud:\n",
        "  - Vertex AI (Imagen)\n",
        "\n",
        "- Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvKl-BtQTRiQ"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwFMpIMrTV_4"
      },
      "source": [
        "### Install Vertex AI SDK for Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYUu8VMdJs3V"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet --upgrade --user google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart current runtime (Jupyter only)\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, it is recommended to restart the runtime. Run the following cell to restart the current kernel.\n",
        "\n",
        "The restart process might take a minute or so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIvVfyyhTPKi"
      },
      "source": [
        "After the restart is complete, continue to the next step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opUxT_k5TdgP"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbNgv4q1T2Mi"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybBXSukZkgjg"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and enable the Vertex AI API.\n",
        "\n",
        "Learn more about setting up a project and a development environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gUjJ42Nh5kf"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1KrdCIGG6uZ"
      },
      "source": [
        "### Import libraries\n",
        " Run the cell below before proceeding to import libraries and define utility functions.\n",
        "You will also load the imagegeneration@006 model from the Vertex AI SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju_PctW22NUl"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import math\n",
        "from typing import Any, List\n",
        "\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "from vertexai.preview.vision_models import Image as Vertex_Image\n",
        "from vertexai.preview.vision_models import (ImageGenerationModel,\n",
        "                                            ImageGenerationResponse)\n",
        "\n",
        "\n",
        "# Gets the image bytes from a PIL Image object.\n",
        "def get_bytes_from_pil(image: Image) -> bytes:\n",
        "    byte_io_png = io.BytesIO()\n",
        "    image.save(byte_io_png, \"PNG\")\n",
        "    return byte_io_png.getvalue()\n",
        "\n",
        "\n",
        "# Corrects the orientation of an image if needed.\n",
        "def maybe_rotate(img_pil):\n",
        "    exif = img_pil.getexif()\n",
        "    rotation = exif.get(274)\n",
        "\n",
        "    if rotation == 3:\n",
        "        img_pil = img_pil.rotate(180, expand=True)\n",
        "    elif rotation == 6:\n",
        "        img_pil = img_pil.rotate(270, expand=True)\n",
        "    elif rotation == 8:\n",
        "        img_pil = img_pil.rotate(90, expand=True)\n",
        "    return img_pil\n",
        "\n",
        "\n",
        "# Extract bounding boxes from a mask.\n",
        "def get_bbox_from_mask(mask_image: np.ndarray, box_area_thres: int = 50) -> np.ndarray:\n",
        "    \"\"\"Finds the contours from a mask image.\"\"\"\n",
        "    contours, _ = cv.findContours(mask_image, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    bboxes = np.zeros((len(contours), 4))\n",
        "    for i, contour in enumerate(contours):\n",
        "        x, y, w, h = cv.boundingRect(contour)\n",
        "        bboxes[i] = (x, y, x + w, y + h)\n",
        "    bboxes = filter(lambda x: (x[2] - x[0]) * (x[3] - x[1]) > box_area_thres, bboxes)\n",
        "    bboxes = sorted(bboxes, key=lambda x: (x[2] - x[0]) * (x[3] - x[1]), reverse=True)\n",
        "    return bboxes\n",
        "\n",
        "\n",
        "# Edits specific areas and pastes them back into the original image.\n",
        "def crop_insert_paste(\n",
        "    generation_model: ImageGenerationModel,\n",
        "    image: Image,\n",
        "    mask: Image,\n",
        "    boxes: np.array,\n",
        "    pad_ratio: int,\n",
        "    prompt: str,\n",
        "    neg_prompt: str,\n",
        "    seed: int = 0,\n",
        "    mask_dilation: float = 0.01,\n",
        "    guidance_scale: int = 60,\n",
        "    samples: int = 4,\n",
        "):\n",
        "    generated_imgs = [image.copy() for _ in range(samples)]\n",
        "    for box in boxes:\n",
        "        # Calculate cropping area with padding.\n",
        "        box_w_pad = pad_ratio * (box[2] - box[0])\n",
        "        box_h_pad = pad_ratio * (box[3] - box[1])\n",
        "        x1 = np.round(np.clip(box[0] - box_w_pad, 0, image.width)).astype(\"int\")\n",
        "        x2 = np.round(np.clip(box[2] + box_w_pad, 0, image.width)).astype(\"int\")\n",
        "        y1 = np.round(np.clip(box[1] - box_h_pad, 0, image.height)).astype(\"int\")\n",
        "        y2 = np.round(np.clip(box[3] + box_h_pad, 0, image.height)).astype(\"int\")\n",
        "\n",
        "        im_crop = image.crop([x1, y1, x2, y2])\n",
        "        mask_crop = mask.crop([x1, y1, x2, y2])\n",
        "        image_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(im_crop))\n",
        "        mask_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(mask_crop))\n",
        "\n",
        "        # Edit the cropped area of the image.\n",
        "        generated_crops = generation_model.edit_image(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=neg_prompt,\n",
        "            base_image=image_vertex,\n",
        "            mask=mask_vertex,\n",
        "            number_of_images=samples,\n",
        "            edit_mode=\"inpainting-insert\",\n",
        "            seed=seed,\n",
        "            guidance_scale=guidance_scale,\n",
        "            mask_dilation=mask_dilation,\n",
        "        )\n",
        "\n",
        "        # Paste the generated edits of the cropped area into the corresponding\n",
        "        # positions in the base image.\n",
        "        for i, crop in enumerate(generated_crops.images):\n",
        "            generated_imgs[i].paste(crop._pil_image, (x1, y1))\n",
        "    return generated_imgs\n",
        "\n",
        "\n",
        "# Pads an image for outpainting. Provides options to control the positioning of\n",
        "# the original image.\n",
        "def pad_to_target_size(\n",
        "    source_image,\n",
        "    target_size=(1536, 1536),\n",
        "    mode=\"RGB\",\n",
        "    vertical_offset_ratio=0,\n",
        "    horizontal_offset_ratio=0,\n",
        "    fill_val=255,\n",
        "):\n",
        "    orig_image_size_w, orig_image_size_h = source_image.size\n",
        "    target_size_w, target_size_h = target_size\n",
        "\n",
        "    insert_pt_x = (target_size_w - orig_image_size_w) // 2 + int(\n",
        "        horizontal_offset_ratio * target_size_w\n",
        "    )\n",
        "    insert_pt_y = (target_size_h - orig_image_size_h) // 2 + int(\n",
        "        vertical_offset_ratio * target_size_h\n",
        "    )\n",
        "    insert_pt_x = min(insert_pt_x, target_size_w - orig_image_size_w)\n",
        "    insert_pt_y = min(insert_pt_y, target_size_h - orig_image_size_h)\n",
        "\n",
        "    if mode == \"RGB\":\n",
        "        source_image_padded = Image.new(\n",
        "            mode, target_size, color=(fill_val, fill_val, fill_val)\n",
        "        )\n",
        "    elif mode == \"L\":\n",
        "        source_image_padded = Image.new(mode, target_size, color=(fill_val))\n",
        "    else:\n",
        "        raise ValueError(\"source image mode must be RGB or L.\")\n",
        "\n",
        "    source_image_padded.paste(source_image, (insert_pt_x, insert_pt_y))\n",
        "    return source_image_padded\n",
        "\n",
        "\n",
        "# Pads and resizes image and mask to the same target size.\n",
        "def pad_image_and_mask(\n",
        "    image_vertex: Vertex_Image,\n",
        "    mask_vertex: Vertex_Image,\n",
        "    target_size,\n",
        "    vertical_offset_ratio,\n",
        "    horizontal_offset_ratio,\n",
        "    viz=True,\n",
        "):\n",
        "    image_vertex.thumbnail(target_size)\n",
        "    mask_vertex.thumbnail(target_size)\n",
        "\n",
        "    image_vertex = pad_to_target_size(\n",
        "        image_vertex,\n",
        "        target_size=target_size,\n",
        "        mode=\"RGB\",\n",
        "        vertical_offset_ratio=vertical_offset_ratio,\n",
        "        horizontal_offset_ratio=horizontal_offset_ratio,\n",
        "        fill_val=0,\n",
        "    )\n",
        "    mask_vertex = pad_to_target_size(\n",
        "        mask_vertex,\n",
        "        target_size=target_size,\n",
        "        mode=\"L\",\n",
        "        vertical_offset_ratio=vertical_offset_ratio,\n",
        "        horizontal_offset_ratio=horizontal_offset_ratio,\n",
        "        fill_val=255,\n",
        "    )\n",
        "    if viz:\n",
        "        print(\n",
        "            f\"image size(with x height): {image_vertex.size[0]} x {image_vertex.size[1]}\"\n",
        "        )\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(image_vertex)\n",
        "        plt.imshow(mask_vertex, alpha=0.3)\n",
        "        plt.title(\"Padded image and mask overlay\")\n",
        "    return image_vertex, mask_vertex\n",
        "\n",
        "\n",
        "# Displays images in a grid below the cell\n",
        "def display_images_in_grid(images: List[Any]) -> None:\n",
        "    \"\"\"Displays the provided images in a grid format. 4 images per row.\n",
        "\n",
        "    Args:\n",
        "        images: A list of images to display.\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine the number of rows and columns for the grid layout.\n",
        "    nrows: int = math.ceil(len(images) / 4)  # Display at most 4 images per row\n",
        "    ncols: int = min(len(images) + 1, 4)  # Adjust columns based on the number of images\n",
        "\n",
        "    # Create a figure and axes for the grid layout.\n",
        "    _, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 6))\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < len(images):\n",
        "            # Display the image in the current axis.\n",
        "            if hasattr(images[i], \"_pil_image\"):\n",
        "                image = images[i]._pil_image\n",
        "            else:\n",
        "                image = images[i]\n",
        "            ax.imshow(image)\n",
        "\n",
        "            # Adjust the axis aspect ratio to maintain image proportions.\n",
        "            ax.set_aspect(\"equal\")\n",
        "\n",
        "            # Disable axis ticks for a cleaner appearance.\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "        else:\n",
        "            # Hide empty subplots to avoid displaying blank axes.\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "    # Adjust the layout to minimize whitespace between subplots.\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display the figure with the arranged images.\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "IMAGE_GENERATION_MODEL = \"imagegeneration@006\"\n",
        "generation_model = ImageGenerationModel.from_pretrained(IMAGE_GENERATION_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R45VRKWInfQQ"
      },
      "source": [
        "## Select an image to edit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "I9caIrZ7Dek1"
      },
      "outputs": [],
      "source": [
        "# @title Upload an image from local\n",
        "# @markdown Run this cell to enable and select the `Choose files` button. \\\n",
        "# @markdown You can then select an image file from your local device to upload. \\\n",
        "# @markdown Your uploaded image will be resized to 1024x1024 pixels for faster processing.\n",
        "\n",
        "images = files.upload()\n",
        "image_bytes = list(images.values())[0]\n",
        "image_pil = maybe_rotate(Image.open(io.BytesIO(image_bytes))).convert(\"RGB\")\n",
        "image_pil.thumbnail((1024, 1024))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(image_pil)\n",
        "print(f\"image size(with x height): {image_pil.size[0]} x {image_pil.size[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqUv45zZ8UiZ"
      },
      "source": [
        "### Generate with Imagen\n",
        "Use the `generate_images` function with Imagen. All you need is a text prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GYBwQuciCco"
      },
      "outputs": [],
      "source": [
        "PROMPT = \"a deer in a field looking at the camera\"\n",
        "\n",
        "response: ImageGenerationResponse = generation_model.generate_images(\n",
        "    prompt=PROMPT,\n",
        ")\n",
        "\n",
        "INPUT_IMAGE = response.images[0]\n",
        "display_images_in_grid(response.images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jTo3dpGRFQp2"
      },
      "outputs": [],
      "source": [
        "# @title [Optional] Upload a mask from local device.\n",
        "# @markdown Run this cell to enable and select the `Choose files` button. \\\n",
        "# @markdown You can then select a mask image file from your local device to upload. \\\n",
        "# @markdown Your uploaded mask will be resized to 1024x1024 pixels for faster processing.\n",
        "\n",
        "# @markdown Required only for mask-based editing. The mask is an image file that\n",
        "# @markdown specifies which part\n",
        "# @markdown of the image you want to edit. \\\n",
        "# @markdown To edit an image without specifying a mask,\n",
        "# @markdown scroll down to try out the MaskMode section in this notebook.\n",
        "\n",
        "masks = files.upload()\n",
        "mask_bytes = list(masks.values())[0]\n",
        "mask_pil = Image.open(io.BytesIO(mask_bytes)).convert(\"L\")\n",
        "mask_pil.thumbnail((1024, 1024))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(mask_pil)\n",
        "print(f\"mask size(with x height): {mask_pil.size[0]} x {mask_pil.size[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhFBZBkMHYbr"
      },
      "source": [
        "## Edit images with a mask\n",
        "Use the `edit_image` function to edit a base image. You can set the following parameters:\n",
        "\n",
        "- `prompt` - A text prompt describing the object to insert.\n",
        "- `negative_prompt` - [Optional] - Define what you DON'T want to see.\n",
        "- `samples` - The number of edited images to generate. An integer between 1 to 4.\n",
        "- `mask_dilation` - The dilation percentage of the mask. Dilation is useful for reducing artifacts around mask boundaries or setting the mask sensitivity for thin objects (e.g. spokes on a bicycle wheel). A decimal value between 0 and 1 and default at 0.01 or `inpainting-insert`.\n",
        "\n",
        "- `guidance_scale` - Controls how much the model adheres to the text prompt. Large values increase output and prompt alignment, but may compromise image quality. A value between 0 to 500, default at 60.\n",
        "- `seed` - Pseudo random seed for reproducible generated outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN_TNaLWKvA5"
      },
      "source": [
        "### Insert objects into an image\n",
        "\n",
        "With the `inpainting-insert` edit mode, you can insert new content into an image.\n",
        "\n",
        "Best practices:\n",
        " - If providing a mask, use medium to large masks.\n",
        " - The edits should be reasonable in regards to lighting, placement, and creating a realistic scene.\n",
        " - Use related context (e.g., natural object interactions, object size reference, composition) in the text prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNCgeYZNJEwz"
      },
      "outputs": [],
      "source": [
        "PROMPT = \"<add your prompt here>\"\n",
        "NEGATIVE_PROMPT = \"\"\n",
        "SAMPLES = 4\n",
        "MASK_DILATION = 0.01\n",
        "GUIDANCE_SCALE = 60\n",
        "SEED = 0\n",
        "\n",
        "image_pil_insert = image_pil\n",
        "mask_pil_insert = mask_pil\n",
        "\n",
        "image_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(image_pil_insert))\n",
        "mask_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(mask_pil_insert))\n",
        "generated_images = generation_model.edit_image(\n",
        "    prompt=PROMPT,\n",
        "    negative_prompt=NEGATIVE_PROMPT,\n",
        "    base_image=image_vertex,\n",
        "    mask=mask_vertex,\n",
        "    number_of_images=SAMPLES,\n",
        "    edit_mode=\"inpainting-insert\",\n",
        "    seed=SEED,\n",
        "    guidance_scale=GUIDANCE_SCALE,\n",
        "    mask_dilation=MASK_DILATION,\n",
        ")\n",
        "\n",
        "display_images_in_grid(generated_images.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCDEDPYwMyr4"
      },
      "source": [
        "#### Special case: inpainting-insert for small objects\n",
        "\n",
        "For some small objects, you can get superior results with this trick.\n",
        "\n",
        "1. crop RGB image and mask, set a prompt that describes the area to be edited.\n",
        "2. use the cropped image and mask to edit the image.\n",
        "3. paste the generated edited images back into the original image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KPGbxZGMvuY"
      },
      "outputs": [],
      "source": [
        "image_pil_insert = image_pil\n",
        "mask_pil_insert = mask_pil\n",
        "\n",
        "bboxes = get_bbox_from_mask(np.array(mask_pil_insert))\n",
        "generated_images = crop_insert_paste(\n",
        "    generation_model,\n",
        "    image_pil_insert,\n",
        "    mask_pil_insert,\n",
        "    bboxes,\n",
        "    pad_ratio=0.5,\n",
        "    prompt=PROMPT,\n",
        "    neg_prompt=NEGATIVE_PROMPT,\n",
        "    samples=SAMPLES,\n",
        "    mask_dilation=MASK_DILATION,\n",
        "    seed=SEED,\n",
        "    guidance_scale=GUIDANCE_SCALE,\n",
        ")\n",
        "\n",
        "display_images_in_grid(generated_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh3ZTaT2N7ku"
      },
      "source": [
        "### Remove objects in an image\n",
        "\n",
        "Set the `edit_mode` to `inpainting-remove` to remove objects in an image.\n",
        "\n",
        "Best practices:\n",
        " - Uses small / medium size masks. Using large size masks is similar to generating new objects\n",
        " - When the target objects throw a shadow, including the shadow in the mask significantly improves result quality. Omitting shadows from input masks provides a cue for the model to generate a corresponding object to the shadow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxaNm59-O7bN"
      },
      "outputs": [],
      "source": [
        "# A text prompt and negative prompt for `inpainting-remove` are not required.\n",
        "# You can set the prompt argument to an empty string.\n",
        "PROMPT = \"\"\n",
        "SAMPLES = 4\n",
        "MASK_DILATION = 0.03\n",
        "GUIDANCE_SCALE = 60\n",
        "SEED = 0\n",
        "\n",
        "image_pil_remove = image_pil\n",
        "mask_pil_remove = mask_pil\n",
        "\n",
        "image_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(image_pil_remove))\n",
        "mask_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(mask_pil_remove))\n",
        "generated_images = generation_model.edit_image(\n",
        "    prompt=PROMPT,\n",
        "    base_image=image_vertex,\n",
        "    mask=mask_vertex,\n",
        "    number_of_images=SAMPLES,\n",
        "    edit_mode=\"inpainting-remove\",\n",
        "    seed=SEED,\n",
        "    guidance_scale=GUIDANCE_SCALE,\n",
        "    mask_dilation=MASK_DILATION,\n",
        ")\n",
        "\n",
        "display_images_in_grid(generated_images.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un1dJrLEQWtn"
      },
      "source": [
        "### Expand an image outwards\n",
        "\n",
        "Use the `outpainting` edit mode to expand an image content beyond its boundaries.\n",
        "\n",
        "Best practices:\n",
        "- The model performs the best when the area to be expanded is not too large.\n",
        "- The textures in the base image should be relatively smooth/not gritty.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiZSgH_bVp5M"
      },
      "source": [
        "#### Prepare outpainting image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCAuoo8iUtGF"
      },
      "outputs": [],
      "source": [
        "# Set rotation degree. Positive degrees to rotate the image anti-clockwise, negative degrees for clockwise.\n",
        "rotation_deg = 0\n",
        "# Pad input image and mask to new size with *target_size_h* as height.\n",
        "target_size_h = 1536\n",
        "# Set the width-to-height aspect ratio for the outpainted images.\n",
        "width_to_height_aspect_ratio = \"3/4\"\n",
        "# Move input image *target_size* x *vertical_offset_ratio* pixels vertically, positive value means downward \\\n",
        "vertical_offset_ratio = 0\n",
        "# Move input image *target_size* x *horizontal_offset_ratio* pixels horizontally, positive value moves image to the right.\n",
        "horizontal_offset_ratio = 0\n",
        "# Visualize padded image and mask.\n",
        "viz = True\n",
        "\n",
        "# Prepare the outpainting input image and mask\n",
        "rot_img = image_pil.rotate(rotation_deg, expand=1, resample=Image.Resampling.BICUBIC)\n",
        "rot_mask = mask_pil.rotate(rotation_deg, expand=1, fillcolor=255)\n",
        "target_size_w = int(target_size_h * eval(width_to_height_aspect_ratio))\n",
        "target_size = (target_size_w, target_size_h)\n",
        "\n",
        "image_pil_outpaint, mask_pil_outpaint = pad_image_and_mask(\n",
        "    rot_img,\n",
        "    rot_mask,\n",
        "    target_size,\n",
        "    vertical_offset_ratio,\n",
        "    horizontal_offset_ratio,\n",
        "    viz=viz,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVKnj_yvVxSD"
      },
      "source": [
        "#### Outpaint images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaVCqqMbVwzX"
      },
      "outputs": [],
      "source": [
        "# [Optional]Set a positive prompt to define what you want to see in outpainted region,\n",
        "# If left empty, the model takes the context cue from the base image.\n",
        "PROMPT = \"<add your prompt here>\"\n",
        "# [Optional] Set a negative prompt to define what you don't want to see.\n",
        "NEGATIVE_PROMPT = \"\"\n",
        "SAMPLES = 4\n",
        "# Default at 0.03 for `outpainting`.\n",
        "MASK_DILATION = 0.03\n",
        "GUIDANCE_SCALE = 60\n",
        "SEED = 0\n",
        "\n",
        "image_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(image_pil_outpaint))\n",
        "mask_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(mask_pil_outpaint))\n",
        "generated_images = generation_model.edit_image(\n",
        "    prompt=PROMPT,\n",
        "    negative_prompt=NEGATIVE_PROMPT,\n",
        "    base_image=image_vertex,\n",
        "    mask=mask_vertex,\n",
        "    number_of_images=SAMPLES,\n",
        "    edit_mode=\"outpainting\",\n",
        "    seed=SEED,\n",
        "    guidance_scale=GUIDANCE_SCALE,\n",
        "    mask_dilation=MASK_DILATION,\n",
        ")\n",
        "\n",
        "display_images_in_grid(generated_images.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_zmAGkoWf2y"
      },
      "source": [
        "### Generate a new background for product images\n",
        "\n",
        "Use Imagen's `product-image` edit mode to set a new background for product images. Imagen excels at generating high image quality background content that fits the lighting and shadows of the product - perfect for advertisements and online shopping. The `product-image` mode always returns 4 1024x1024 images.\n",
        "\n",
        "Best practices:\n",
        "\n",
        "- We will generate a foreground product mask for you. Avoid using products or objects that have thin, isolated parts, e.g. long handbag straps, a single golf club.\n",
        "- Put desired objects closer to the bottom of a square input image. This is achieved by pre-padding objects in a square input image and setting product_position to `fixed` in the API.\n",
        "\n",
        "You can use the `product_position` parameter to set how the primary product is placed in the generated images.\n",
        "\n",
        "*   `reposition`- Re-centers the primary product in the generated image.\n",
        "*   `fixed`- For square input images, this will keep the primary product in the same relative location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmwqAj1YWXZI"
      },
      "outputs": [],
      "source": [
        "# Set a text prompt that describes the new image you want to see with the product.\n",
        "PROMPT = \"<add your prompt here>\"\n",
        "# Recenter the primary product in the generated image\n",
        "PRODUCT_POSITION = \"reposition\"\n",
        "\n",
        "image_vertex = Vertex_Image(image_bytes=get_bytes_from_pil(image_pil))\n",
        "generated_images = generation_model.edit_image(\n",
        "    prompt=PROMPT,\n",
        "    base_image=image_vertex,\n",
        "    edit_mode=\"product-image\",\n",
        "    product_position=PRODUCT_POSITION,\n",
        ")\n",
        "\n",
        "display_images_in_grid(generated_images.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU32286ooc8Q"
      },
      "source": [
        "## Edit images using MaskMode\n",
        "\n",
        "Now you can edit images without providing your own mask. Configure the `mask_mode` field `edit_image` request to automatically generate a mask on the input image.\n",
        "\n",
        "MaskMode provides the following modes:\n",
        "* **Background**: Edit the background of an image\n",
        "* **Foreground**: Edit the foreground of an image\n",
        "* **Semantic**: Edit specified objects in an image. You can edit 1 to 5 objects in an image using semantic segmentation classes.\n",
        "\n",
        "The `semantic` maskMode option requires you to set **Segmentation classes**. You must set 1 to 5 classes using the desired class ID. The full table of available classes is listed in the `Appendix` section at the end of this Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBLJtICO8iMQ"
      },
      "source": [
        "### Explore different MaskMode options\n",
        "\n",
        "This section will explores how to edit images using different `edit_mode` and `mask_mode` parameter options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRXg5vk-p3GN"
      },
      "outputs": [],
      "source": [
        "# Set the edit mode. Choose from [\"inpainting-insert\", \"inpainting-remove\", \"outpainting\"].\n",
        "EDIT_MODE = \"inpainting-insert\"\n",
        "# Set how the mask should be generated. Choose from [\"background\", \"foreground\", \"semantic\"].\n",
        "MASK_MODE = \"foreground\"\n",
        "# Specify an object to edit using a segmentation class. Only valid for `semantic` maskMode.\n",
        "SEGMENTATION_CLASS = 16\n",
        "\n",
        "# Set a text prompt to influence how the masked part of the image will be edited.\n",
        "PROMPT = \"a cow looking at the camera\"\n",
        "# [Optional] Set a negative prompt to define what you don't want to see.\n",
        "NEGATIVE_PROMPT = \"\"\n",
        "\n",
        "classes = None\n",
        "if MASK_MODE == \"semantic\":\n",
        "    classes = [SEGMENTATION_CLASS]\n",
        "\n",
        "response: ImageGenerationResponse = generation_model.edit_image(\n",
        "    prompt=PROMPT,\n",
        "    base_image=INPUT_IMAGE,\n",
        "    negative_prompt=NEGATIVE_PROMPT,\n",
        "    number_of_images=4,\n",
        "    edit_mode=EDIT_MODE,\n",
        "    mask_mode=MASK_MODE,\n",
        "    segmentation_classes=classes,\n",
        ")\n",
        "\n",
        "display_images_in_grid(response.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--7rofOb95hT"
      },
      "source": [
        "### Inpainting-insert with Background maskMode\n",
        "\n",
        "Edit the background of an image using a text prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBY_5JWlxojZ"
      },
      "outputs": [],
      "source": [
        "EDIT_MODE = \"inpainting-insert\"\n",
        "MASK_MODE = \"background\"\n",
        "# The background will be edited to adhere to the text prompt below.\n",
        "PROMPT = \"sandy desert oasis\"\n",
        "# [Optional] Set a negative prompt to define what you don't want to see.\n",
        "NEGATIVE_PROMPT = \"\"\n",
        "\n",
        "response: ImageGenerationResponse = generation_model.edit_image(\n",
        "    prompt=PROMPT,\n",
        "    base_image=INPUT_IMAGE,\n",
        "    negative_prompt=NEGATIVE_PROMPT,\n",
        "    edit_mode=EDIT_MODE,\n",
        "    mask_mode=MASK_MODE,\n",
        "    segmentation_classes=classes,\n",
        ")\n",
        "\n",
        "display_images_in_grid(response.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4YrxGDE-OY9"
      },
      "source": [
        "### Inpainting-insert with Foreground maskMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atoD2gTKycAp"
      },
      "outputs": [],
      "source": [
        "EDIT_MODE = \"inpainting-insert\"\n",
        "MASK_MODE = \"foreground\"\n",
        "\n",
        "# The foreground of the object will be edited according to the text prompt below.\n",
        "PROMPT = \"a bear looking at the camera\"\n",
        "# [Optional] Set a negative prompt to define what you don't want to see.\n",
        "NEGATIVE_PROMPT = \"\"\n",
        "\n",
        "response: ImageGenerationResponse = generation_model.edit_image(\n",
        "    prompt=PROMPT,\n",
        "    base_image=INPUT_IMAGE,\n",
        "    negative_prompt=NEGATIVE_PROMPT,\n",
        "    edit_mode=EDIT_MODE,\n",
        "    mask_mode=MASK_MODE,\n",
        "    segmentation_classes=classes,\n",
        ")\n",
        "\n",
        "display_images_in_grid(response.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XXCDNGz-ew2"
      },
      "source": [
        "### Inpainting-insert with Semantic maskMode\n",
        "\n",
        "Edit a specified object or multiple objects in an image using Semantic maskMode.\n",
        "You must set between 1 and 5 IDs in the `segmentation_classes` field. The full\n",
        "list of available segmentation classes is listed in the Appendix section at the bottom of this Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFL0Hh_9ycXj"
      },
      "outputs": [],
      "source": [
        "EDIT_MODE = \"inpainting-insert\"\n",
        "MASK_MODE = \"semantic\"\n",
        "\n",
        "# Set the specified object(s) to edit in an image using a segmentation class.\n",
        "SEGMENTATION_CLASS = 16\n",
        "\n",
        "PROMPT = \"A cow looking at the camera\"\n",
        "# [Optional] Set a negative prompt to define what you don't want to see.\n",
        "NEGATIVE_PROMPT = \"\"\n",
        "\n",
        "response: ImageGenerationResponse = generation_model.edit_image(\n",
        "    prompt=PROMPT,\n",
        "    base_image=INPUT_IMAGE,\n",
        "    negative_prompt=NEGATIVE_PROMPT,\n",
        "    edit_mode=EDIT_MODE,\n",
        "    mask_mode=MASK_MODE,\n",
        "    segmentation_classes=[SEGMENTATION_CLASS],\n",
        ")\n",
        "\n",
        "display_images_in_grid(response.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbbpshyY_Hey"
      },
      "source": [
        "### Inpainting-remove with foreground maskMode\n",
        "\n",
        "Remove the foreground object of an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_I2QlwQz0I2"
      },
      "outputs": [],
      "source": [
        "EDIT_MODE = \"inpainting-remove\"\n",
        "MASK_MODE = \"foreground\"\n",
        "PROMPT = \"Background, landscape photo\"\n",
        "# [Optional] Set a negative prompt to define what you don't want to see.\n",
        "NEGATIVE_PROMPT = \"\"\n",
        "\n",
        "response: ImageGenerationResponse = generation_model.edit_image(\n",
        "    prompt=PROMPT,\n",
        "    base_image=INPUT_IMAGE,\n",
        "    edit_mode=EDIT_MODE,\n",
        "    mask_mode=MASK_MODE,\n",
        "    segmentation_classes=classes,\n",
        ")\n",
        "\n",
        "display_images_in_grid(response.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PixKFo0h_SZj"
      },
      "source": [
        "### Inpainting-remove with Semantic maskMode\n",
        "\n",
        "Remove the specified object(s) in an image using a segmentation class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9ZV33281192"
      },
      "outputs": [],
      "source": [
        "EDIT_MODE = \"inpainting-remove\"\n",
        "MASK_MODE = \"semantic\"\n",
        "# Set the object that will be removed according to its segmentation class ID.\n",
        "SEGMENTATION_CLASS = 125\n",
        "\n",
        "# Set a text prompt to guide the edited image.\n",
        "PROMPT = \"Background, landscape photo\"\n",
        "# Set a negative prompt to define what you don't want to see.\n",
        "NEGATIVE_PROMPT = \"\"\n",
        "\n",
        "response: ImageGenerationResponse = generation_model.edit_image(\n",
        "    prompt=PROMPT,\n",
        "    base_image=INPUT_IMAGE,\n",
        "    negative_prompt=NEGATIVE_PROMPT,\n",
        "    edit_mode=EDIT_MODE,\n",
        "    mask_mode=MASK_MODE,\n",
        "    segmentation_classes=[SEGMENTATION_CLASS],\n",
        ")\n",
        "\n",
        "display_images_in_grid(response.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk0eXjQ1nR4F"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "You have explored the Imagen's image editing features through the Vertex AI Python SDK, including the additional parameters that influence image generation.\n",
        "\n",
        "Check out the Vertex AI reference to learn more about how to [Edit image prompts](https://cloud.google.com/vertex-ai/generative-ai/docs/image/img-gen-prompt-guide#edit-prompts)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuaLTarf-hvO"
      },
      "source": [
        "## Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZzRAwQ0dIjT"
      },
      "source": [
        "### Semantic segmentation classes\n",
        "\n",
        "| Class ID | Instance Type | Class ID | Instance Type | Class ID | Instance Type | Class ID | Instance Type |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| 0 | backpack | 50 | carrot | 100 | sidewalk_pavement | 150 | skis |\n",
        "| 1 | umbrella | 51 | hot_dog | 101 | runway | 151 | snowboard |\n",
        "| 2 | bag | 52 | pizza | 102 | terrain | 152 | sports_ball |\n",
        "| 3 | tie | 53 | donut | 103 | book | 153 | kite |\n",
        "| 4 | suitcase | 54 | cake | 104 | box | 154 | baseball_bat |\n",
        "| 5 | case | 55 | fruit_other | 105 | clock | 155 | baseball_glove |\n",
        "| 6 | bird | 56 | food_other | 106 | vase | 156 | skateboard |\n",
        "| 7 | cat | 57 | chair_other | 107 | scissors | 157 | surfboard |\n",
        "| 8 | dog | 58 | armchair | 108 | plaything_other | 158 | tennis_racket |\n",
        "| 9 | horse | 59 | swivel_chair | 109 | teddy_bear | 159 | net |\n",
        "| 10 | sheep | 60 | stool | 110 | hair_dryer | 160 | base |\n",
        "| 11 | cow | 61 | seat | 111 | toothbrush | 161 | sculpture |\n",
        "| 12 | elephant | 62 | couch | 112 | painting | 162 | column |\n",
        "| 13 | bear | 63 | trash_can | 113 | poster | 163 | fountain |\n",
        "| 14 | zebra | 64 | potted_plant | 114 | bulletin_board | 164 | awning |\n",
        "| 15 | giraffe | 65 | nightstand | 115 | bottle | 165 | apparel |\n",
        "| 16 | animal_other | 66 | bed | 116 | cup | 166 | banner |\n",
        "| 17 | microwave | 67 | table | 117 | wine_glass | 167 | flag |\n",
        "| 18 | radiator | 68 | pool_table | 118 | knife | 168 | blanket |\n",
        "| 19 | oven | 69 | barrel | 119 | fork | 169 | curtain_other |\n",
        "| 20 | toaster | 70 | desk | 120 | spoon | 170 | shower_curtain |\n",
        "| 21 | storage_tank | 71 | ottoman | 121 | bowl | 171 | pillow |\n",
        "| 22 | conveyor_belt | 72 | wardrobe | 122 | tray | 172 | towel |\n",
        "| 23 | sink | 73 | crib | 123 | range_hood | 173 | rug_floormat |\n",
        "| 24 | refrigerator | 74 | basket | 124 | plate | 174 | vegetation |\n",
        "| 25 | washer_dryer | 75 | chest_of_drawers | 125 | person | 175 | bicycle |\n",
        "| 26 | fan | 76 | bookshelf | 126 | rider_other | 176 | car |\n",
        "| 27 | dishwasher | 77 | counter_other | 127 | bicyclist | 177 | autorickshaw |\n",
        "| 28 | toilet | 78 | bathroom_counter | 128 | motorcyclist | 178 | motorcycle |\n",
        "| 29 | bathtub | 79 | kitchen_island | 129 | paper | 179 | airplane |\n",
        "| 30 | shower | 80 | door | 130 | streetlight | 180 | bus |\n",
        "| 31 | tunnel | 81 | light_other | 131 | road_barrier | 181 | train |\n",
        "| 32 | bridge | 82 | lamp | 132 | mailbox | 182 | truck |\n",
        "| 33 | pier_wharf | 83 | sconce | 133 | cctv_camera | 183 | trailer |\n",
        "| 34 | tent | 84 | chandelier | 134 | junction_box | 184 | boat_ship |\n",
        "| 35 | building | 85 | mirror | 135 | traffic_sign | 185 | slow_wheeled_object |\n",
        "| 36 | ceiling | 86 | whiteboard | 136 | traffic_light | 186 | river_lake |\n",
        "| 37 | laptop | 87 | shelf | 137 | fire_hydrant | 187 | sea |\n",
        "| 38 | keyboard | 88 | stairs | 138 | parking_meter | 188 | water_other |\n",
        "| 39 | mouse | 89 | escalator | 139 | bench | 189 | swimming_pool |\n",
        "| 40 | remote | 90 | cabinet | 140 | bike_rack | 190 | waterfall |\n",
        "| 41 | cell phone | 91 | fireplace | 141 | billboard | 191 | wall |\n",
        "| 42 | television | 92 | stove | 142 | sky | 192 | window |\n",
        "| 43 | floor | 93 | arcade_machine | 143 | pole | 193 | window_blind |\n",
        "| 44 | stage | 94 | gravel | 144 | fence | | |\n",
        "| 45 | banana | 95 | platform | 145 | railing_banister | | |\n",
        "| 46 | apple | 96 | playingfield | 146 | guard_rail | | |\n",
        "| 47 | sandwich | 97 | railroad | 147 | mountain_hill | | |\n",
        "| 48 | orange | 98 | road | 148 | rock | | |\n",
        "| 49 | broccoli | 99 | snow | 149 | frisbee | | |\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "image_editing.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
